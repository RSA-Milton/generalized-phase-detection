{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4311,
     "status": "ok",
     "timestamp": 1758207547701,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "1_Q-y8sPNgq0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1442,
     "status": "ok",
     "timestamp": 1757714680013,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "0eZFs4poQ7qV",
    "outputId": "a816b7a6-e25c-40fc-ae6e-de395fa6d5d4"
   },
   "outputs": [],
   "source": [
    "# Cargar el dataset (ajusta la ruta a tu archivo)\n",
    "filename = '/home/rsa/projects/gpd/data/raw/datasets/analyst_picks.csv'\n",
    "df = pd.read_csv(filename, sep=\";\")\n",
    "\n",
    "# Extraer el código de estación (antes del primer \"_\")\n",
    "df.insert(0, \"Estacion\", df[\"mseed\"].str.split(\"_\").str[0])\n",
    "\n",
    "# Verificar los primeros registros\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1757625311175,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "8cAeBaba1BBd",
    "outputId": "17fdd31d-d3c6-4dac-ffe4-eb6f4aaefa2d"
   },
   "outputs": [],
   "source": [
    "# Guardar archivo modificado\n",
    "OUT_CSV = '/home/rsa/projects/gpd/data/raw/datasets/dataset.csv'\n",
    "df.to_csv(OUT_CSV, index=False, sep=\";\", encoding=\"utf-8-sig\")\n",
    "print(\"\\nGuardado en:\", OUT_CSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 842
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1757714690095,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "xQeu02gFZSVP",
    "outputId": "8671a285-5a57-4668-90f8-d169c62b36fa"
   },
   "outputs": [],
   "source": [
    "# Listar estaciones unicas\n",
    "estaciones = df[\"Estacion\"].unique()\n",
    "print(\"Estaciones encontradas:\")\n",
    "print(estaciones)\n",
    "print(\"\\nNumero total de estaciones:\", len(estaciones))\n",
    "# Contar el total de registros por estacion\n",
    "conteo_estaciones = df[\"Estacion\"].value_counts()\n",
    "# Mostrar en tabla\n",
    "print(conteo_estaciones)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10,5))\n",
    "conteo_estaciones.plot(kind=\"bar\")\n",
    "plt.title(\"Total de datos por estación\")\n",
    "plt.xlabel(\"Estación\")\n",
    "plt.ylabel(\"Número de registros\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tpzRH8yXW0rl"
   },
   "source": [
    "**Filtrado**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcKz1tSfb9-K"
   },
   "source": [
    "Filtrado de estaciones con fases ponderadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 719
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1757625941310,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "bdt3cN1naAub",
    "outputId": "5df1c9d5-57bb-4308-e72c-ef9867ae2b6f"
   },
   "outputs": [],
   "source": [
    "# Eliminar filas donde ambas ponderaciones (P y S) son NA\n",
    "df_filtrado_ponderado = df.dropna(subset=[\"Pond T-P\", \"Pond T-S\"], how=\"all\")\n",
    "\n",
    "# Mostrar número de filas resultantes\n",
    "print(\"Total de filas resultantes:\", len(df_filtrado_ponderado))\n",
    "\n",
    "# Listar estaciones unicas\n",
    "estaciones = df_filtrado_ponderado[\"Estacion\"].unique()\n",
    "print(\"Estaciones encontradas:\")\n",
    "print(estaciones)\n",
    "print(\"\\nNumero total de estaciones:\", len(estaciones))\n",
    "# Contar el total de registros por estacion\n",
    "conteo_estaciones = df_filtrado_ponderado[\"Estacion\"].value_counts()\n",
    "# Mostrar en tabla\n",
    "print(conteo_estaciones)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10,5))\n",
    "conteo_estaciones.plot(kind=\"bar\")\n",
    "plt.title(\"Total de datos por estación\")\n",
    "plt.xlabel(\"Estación\")\n",
    "plt.ylabel(\"Número de registros\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dnb5oCBRZ2we"
   },
   "source": [
    "Filtrado de estaciones de 64 Hz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1757269822467,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "KAEI6W4fcNSB",
    "outputId": "1d4eea5f-7df8-4b96-a448-0ef686cce0b7"
   },
   "outputs": [],
   "source": [
    "# Filtrar filas con Muestreo = 64.0\n",
    "df_filtrado_64 = df_filtrado_ponderado[df_filtrado_ponderado[\"Muestreo\"] == 64.0]\n",
    "# Mostrar número de filas resultantes\n",
    "print(\"Total de filas resultantes:\", len(df_filtrado_64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYVlgvdFgJjy"
   },
   "source": [
    "Filtrado de estaciones que tengan unicamente la ponderacion P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 54,
     "status": "ok",
     "timestamp": 1757271005055,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "cX9h6ObngH1s",
    "outputId": "16f6288a-ab1a-4a32-e29b-d452f26b0dc3"
   },
   "outputs": [],
   "source": [
    "# Eliminar filas donde Pond T-S sea NA\n",
    "df_filtrado_p = df_filtrado_ponderado.dropna(subset=[\"Pond T-S\"])\n",
    "# Mostrar número de filas resultantes\n",
    "print(\"Total de filas resultantes:\", len(df_filtrado_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5R3lraZOgjVu"
   },
   "source": [
    "Filtrado de estaciones que tengan unicamente la ponderacion S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1757271006533,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "jZgXx8XCgn4a",
    "outputId": "9a3e82f0-2d7f-4893-c498-a641201b07ad"
   },
   "outputs": [],
   "source": [
    "# Eliminar filas donde Pond T-P sea NA\n",
    "df_filtrado_s = df_filtrado_ponderado.dropna(subset=[\"Pond T-P\"])\n",
    "# Mostrar número de filas resultantes\n",
    "print(\"Total de filas resultantes:\", len(df_filtrado_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1757268142065,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "gYwDuP1eSiHv",
    "outputId": "c04cfc81-b4a8-4e36-8f91-334fdd08f3b7"
   },
   "outputs": [],
   "source": [
    "# Filtrar filas con Muestreo = 64.0\n",
    "df_filtrado = df[df[\"Muestreo\"] == 64.0]\n",
    "\n",
    "# Eliminar filas donde ambas ponderaciones (P y S) son NA\n",
    "#df_filtrado = df_filtrado.dropna(subset=[\"Pond T-P\", \"Pond T-S\"], how=\"all\")\n",
    "\n",
    "# Eliminar filas donde al menos una ponderacion (P o S) sea NA\n",
    "df_filtrado_ps = df_filtrado.dropna(subset=[\"Pond T-P\", \"Pond T-S\"], how=\"any\")\n",
    "\n",
    "# Eliminar filas donde Pond T-P sea NA\n",
    "df_filtrado_s = df_filtrado.dropna(subset=[\"Pond T-P\"])\n",
    "\n",
    "# Eliminar filas donde Pond T-P sea NA\n",
    "df_filtrado_s = df_filtrado.dropna(subset=[\"Pond T-S\"])\n",
    "\n",
    "# Mostrar número de filas resultantes\n",
    "print(\"Total de filas resultantes:\", len(df_filtrado))\n",
    "\n",
    "# Vista previa\n",
    "df_filtrado.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1757235799816,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "WPbasGzJTfxv",
    "outputId": "508c6728-6b4a-4dbd-bec2-3b1a2e71bd71"
   },
   "outputs": [],
   "source": [
    "# Listar estaciones unicas\n",
    "estaciones = df_filtrado[\"Estacion\"].unique()\n",
    "print(\"Estaciones encontradas:\")\n",
    "print(estaciones)\n",
    "\n",
    "print(\"\\nNumero total de estaciones:\", len(estaciones))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 633
    },
    "executionInfo": {
     "elapsed": 190,
     "status": "ok",
     "timestamp": 1757235804712,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "BCbRwj67TzxV",
    "outputId": "cf693133-cb9e-4ada-cecc-9d5d2c40f699"
   },
   "outputs": [],
   "source": [
    "# Contar el total de registros por estacion\n",
    "conteo_estaciones = df_filtrado[\"Estacion\"].value_counts()\n",
    "\n",
    "# Mostrar en tabla\n",
    "print(conteo_estaciones)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10,5))\n",
    "conteo_estaciones.plot(kind=\"bar\")\n",
    "plt.title(\"Total de datos por estación (Muestreo=64.0)\")\n",
    "plt.xlabel(\"Estación\")\n",
    "plt.ylabel(\"Número de registros\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1757235881097,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "5hApyXE-W5OZ",
    "outputId": "bfc837f3-10c0-4ce0-b8e5-60850df1b1af"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========= Parametros =========\n",
    "CSV_PATH = filename      # ruta a tu CSV\n",
    "FREQ = \"W\"                 # granularidad temporal: \"D\" (dia), \"W\" (semana), \"M\" (mes)\n",
    "SEED = 42                  # semilla para reproducibilidad\n",
    "ESTACIONES_OBJ = [\"LABR\", \"CUSH\", \"CHAI\", \"UVER\", \"PORT\"]\n",
    "TARGET_POR_ESTACION = 20  # total por estacion\n",
    "# ==============================\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# 1) Cargar y preparar\n",
    "df = pd.read_csv(CSV_PATH, sep=\";\")\n",
    "# Extraer codigo de estacion\n",
    "df.insert(0, \"Estacion\", df[\"mseed\"].str.split(\"_\").str[0])\n",
    "\n",
    "# Si ya trabajas con df_filtrado, sustituye df por df_filtrado a partir de aqui:\n",
    "# df = df_filtrado.copy()\n",
    "\n",
    "# Asegurar tipos de tiempo\n",
    "df[\"T-ini\"] = pd.to_datetime(df[\"T-ini\"], errors=\"coerce\", utc=True)\n",
    "df[\"T-fin\"] = pd.to_datetime(df[\"T-fin\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# 2) Filtrado opcional (descomenta si quieres mantener exactamente el mismo filtrado previo)\n",
    "df = df[df[\"Muestreo\"] == 64.0]\n",
    "# Eliminar filas donde ambas ponderaciones (P y S) son NA\n",
    "#df = df.dropna(subset=[\"Pond T-P\", \"Pond T-S\"], how=\"all\")\n",
    "# Eliminar filas donde al menos una ponderacion (P o S) sea NA\n",
    "df_filtrado = df_filtrado.dropna(subset=[\"Pond T-P\", \"Pond T-S\"], how=\"any\")\n",
    "\n",
    "# 3) Quedarse con las estaciones objetivo\n",
    "df = df[df[\"Estacion\"].isin(ESTACIONES_OBJ)].copy()\n",
    "\n",
    "# 4) Crear bins temporales (fechas similares por semana/mes/dia segun FREQ)\n",
    "#    Se usa el inicio del periodo como marca del bin\n",
    "df[\"time_bin\"] = df[\"T-ini\"].dt.to_period(FREQ).dt.start_time\n",
    "\n",
    "# 5) Identificar bins comunes a todas las estaciones (interseccion de bins)\n",
    "bins_por_est = {\n",
    "    est: set(df.loc[df[\"Estacion\"] == est, \"time_bin\"].dropna().unique())\n",
    "    for est in ESTACIONES_OBJ\n",
    "}\n",
    "bins_comunes = set.intersection(*bins_por_est.values()) if bins_por_est else set()\n",
    "bins_comunes = sorted(bins_comunes)\n",
    "\n",
    "if len(bins_comunes) == 0:\n",
    "    raise ValueError(\"No hay bins temporales comunes entre todas las estaciones con la frecuencia seleccionada. \"\n",
    "                     \"Prueba con otra FREQ (p.ej., 'M') o relaja filtros.\")\n",
    "\n",
    "# 6) Prealocar cuotas por bin (distribucion lo mas uniforme posible)\n",
    "#    Ej: si hay 40 semanas comunes y se quieren 200 muestras, asigna 5 por semana, con el resto distribuido\n",
    "def cuotas_por_bin(target, num_bins):\n",
    "    base = target // num_bins\n",
    "    resto = target % num_bins\n",
    "    cuotas = np.full(num_bins, base, dtype=int)\n",
    "    # distribuir el resto uno por uno desde el inicio\n",
    "    cuotas[:resto] += 1\n",
    "    return cuotas\n",
    "\n",
    "cuotas_base = cuotas_por_bin(TARGET_POR_ESTACION, len(bins_comunes))\n",
    "\n",
    "# 7) Funcion de muestreo estratificado por bins para una estacion\n",
    "def muestrear_por_bins(df_est, bins_ord, cuotas, rng):\n",
    "    tomadas = []\n",
    "    # intentar cumplir cuotas por cada bin\n",
    "    for bin_val, cuota in zip(bins_ord, cuotas):\n",
    "        if cuota <= 0:\n",
    "            continue\n",
    "        candidatos = df_est[df_est[\"time_bin\"] == bin_val]\n",
    "        if len(candidatos) == 0:\n",
    "            continue\n",
    "        # si hay menos que la cuota, tomar todos; si hay mas, sample\n",
    "        if len(candidatos) <= cuota:\n",
    "            tomadas.append(candidatos)\n",
    "        else:\n",
    "            tomadas.append(candidatos.sample(n=cuota, random_state=rng))\n",
    "    sel = pd.concat(tomadas) if tomadas else df_est.iloc[0:0]\n",
    "    # si no alcanza el total, completar con otros bins (fuera de bins comunes) o sobrantes\n",
    "    faltan = TARGET_POR_ESTACION - len(sel)\n",
    "    if faltan > 0:\n",
    "        # prioridad: tomar de otros registros de la misma estacion no seleccionados\n",
    "        resto = df_est.drop(sel.index)\n",
    "        if len(resto) > 0:\n",
    "            if len(resto) <= faltan:\n",
    "                sel = pd.concat([sel, resto])\n",
    "            else:\n",
    "                sel = pd.concat([sel, resto.sample(n=faltan, random_state=rng)])\n",
    "    return sel.head(TARGET_POR_ESTACION)\n",
    "\n",
    "# 8) Muestrear 200 por estacion\n",
    "muestras_estaciones = []\n",
    "resumen = {}\n",
    "for est in ESTACIONES_OBJ:\n",
    "    df_est = df[df[\"Estacion\"] == est].copy()\n",
    "    sel_est = muestrear_por_bins(df_est, bins_comunes, cuotas_base, rng)\n",
    "    muestras_estaciones.append(sel_est)\n",
    "    resumen[est] = len(sel_est)\n",
    "\n",
    "df_sampled = pd.concat(muestras_estaciones).reset_index(drop=True)\n",
    "\n",
    "# 9) Validaciones\n",
    "conteo_final = df_sampled[\"Estacion\"].value_counts().reindex(ESTACIONES_OBJ, fill_value=0)\n",
    "total_final = len(df_sampled)\n",
    "\n",
    "print(\"Resumen por estacion (esperado 200 c/u):\")\n",
    "print(conteo_final)\n",
    "print(\"\\nTotal esperado:\", TARGET_POR_ESTACION * len(ESTACIONES_OBJ))\n",
    "print(\"Total obtenido:\", total_final)\n",
    "\n",
    "# 10) (Opcional) inspeccion rápida de alineacion temporal\n",
    "print(\"\\nPrimeras fechas por estacion en la muestra:\")\n",
    "print(df_sampled.groupby(\"Estacion\")[\"T-ini\"].min())\n",
    "print(\"\\nUltimas fechas por estacion en la muestra:\")\n",
    "print(df_sampled.groupby(\"Estacion\")[\"T-ini\"].max())\n",
    "\n",
    "# 11) Guardar resultado\n",
    "OUT_CSV = \"/content/drive/MyDrive/Colab Notebooks/TFM/dataset_estratificado_{}_{}.csv\".format(FREQ, TARGET_POR_ESTACION * len(ESTACIONES_OBJ))\n",
    "df_sampled.to_csv(OUT_CSV, index=False)\n",
    "print(\"\\nGuardado en:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hY5JTYdZc4ef"
   },
   "source": [
    "Esta nueva version:\n",
    "\n",
    "Para PORT y UVER toma todo lo disponible (sin NA y a 64 Hz) y no fuerza cuotas por semana/mes; luego, si no llegan a 200, rellena con muestras de LABR/CUSH/CHAI, priorizando los mismos bins temporales donde aparecieron PORT/UVER (para mantener “fechas similares”).\n",
    "\n",
    "Para LABR/CUSH/CHAI si hace una seleccion priorizando los bins de las estaciones pequenas; si falta, completa con cualquier bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4131,
     "status": "ok",
     "timestamp": 1757715169844,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "y97eRaa3c--x",
    "outputId": "33b02900-65f5-4799-c584-c6af386dc09b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========= Rutas en Colab =========\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "IN_CSV  = '/content/drive/MyDrive/Colab Notebooks/TFM/test.csv'\n",
    "OUT_CSV = '/content/drive/MyDrive/Colab Notebooks/TFM/dataset_estratificado.csv'\n",
    "# ==================================\n",
    "\n",
    "# ========= Parametros =========\n",
    "FREQ = \"W\"                 # \"D\" dia, \"W\" semana, \"M\" mes\n",
    "SEED = 42\n",
    "TARGET_POR_EST = 20\n",
    "ESTACIONES_OBJ = [\"LABR\", \"CUSH\", \"CHAI\", \"PORT\", \"UVER\"]\n",
    "SMALL_STATIONS = {\"PORT\", \"UVER\"}\n",
    "# ==============================\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# 1) Cargar y preparar\n",
    "df = pd.read_csv(IN_CSV, sep=\";\")\n",
    "\n",
    "# Estacion\n",
    "df.insert(0, \"Estacion\", df[\"mseed\"].str.split(\"_\").str[0])\n",
    "\n",
    "# Tiempos\n",
    "df[\"T-ini\"] = pd.to_datetime(df[\"T-ini\"], errors=\"coerce\", utc=True)\n",
    "df[\"T-fin\"] = pd.to_datetime(df[\"T-fin\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# 2) Filtros estrictos requeridos\n",
    "df = df[df[\"Muestreo\"] == 64.0].copy()\n",
    "# Sin NA en ponderaciones: ambas deben existir\n",
    "df = df.dropna(subset=[\"Pond T-P\", \"Pond T-S\"], how=\"any\").copy()\n",
    "\n",
    "# 3) Mantener solo estaciones objetivo\n",
    "df = df[df[\"Estacion\"].isin(ESTACIONES_OBJ)].copy()\n",
    "\n",
    "# 4) Bins temporales\n",
    "df[\"time_bin\"] = df[\"T-ini\"].dt.to_period(FREQ).dt.start_time\n",
    "\n",
    "# 5) Separar por estacion\n",
    "by_est = {est: df[df[\"Estacion\"] == est].copy() for est in ESTACIONES_OBJ}\n",
    "\n",
    "# 6) Seleccion para estaciones pequenas: sin criterio temporal, tomar todo hasta TARGET\n",
    "selecciones = {}\n",
    "for est in ESTACIONES_OBJ:\n",
    "  dfe = by_est[est]\n",
    "  if est in SMALL_STATIONS:\n",
    "    if len(dfe) <= TARGET_POR_EST:\n",
    "      selecciones[est] = dfe.copy()\n",
    "    else:\n",
    "      selecciones[est] = dfe.sample(n=TARGET_POR_EST, random_state=rng)  # por si hay mas de 200\n",
    "  else:\n",
    "    selecciones[est] = dfe.iloc[0:0].copy()  # placeholder para grandes\n",
    "\n",
    "# 7) Construir el conjunto de bins a priorizar según PORT y UVER (fechas similares)\n",
    "bins_small = set(pd.concat([selecciones[s] for s in SMALL_STATIONS if s in selecciones and len(selecciones[s]) > 0])[\"time_bin\"].unique())\n",
    "bins_small = sorted([b for b in bins_small if pd.notna(b)])\n",
    "\n",
    "# Si no hay bins en las pequenas (muy extremo), usar los bins mas frecuentes del conjunto total\n",
    "if len(bins_small) == 0:\n",
    "  bins_small = list(df[\"time_bin\"].value_counts().index[:10])  # top 10 como fallback\n",
    "\n",
    "# 8) Asignar cupos a estaciones grandes (LABR, CUSH, CHAI)\n",
    "def cuotas_por_bin(total_objetivo, num_bins):\n",
    "  base = total_objetivo // num_bins if num_bins > 0 else 0\n",
    "  resto = total_objetivo % num_bins if num_bins > 0 else 0\n",
    "  cuotas = np.full(max(num_bins,1), base, dtype=int)\n",
    "  cuotas[:resto] += 1\n",
    "  return cuotas\n",
    "\n",
    "for est in ESTACIONES_OBJ:\n",
    "  if est in SMALL_STATIONS:\n",
    "    continue\n",
    "  dfe = by_est[est].copy()\n",
    "  # objetivo por estacion\n",
    "  objetivo = TARGET_POR_EST\n",
    "\n",
    "  # ya hay seleccion para est? (deberia estar vacia para grandes)\n",
    "  sel_existente = selecciones.get(est, dfe.iloc[0:0].copy())\n",
    "\n",
    "  faltan = max(0, objetivo - len(sel_existente))\n",
    "  if faltan == 0:\n",
    "    continue\n",
    "\n",
    "  # 8a) Intentar cubrir faltantes priorizando bins_small\n",
    "  dfe_restante = dfe.drop(sel_existente.index)\n",
    "  dfe_prior = dfe_restante[dfe_restante[\"time_bin\"].isin(bins_small)].copy()\n",
    "\n",
    "  if len(dfe_prior) >= faltan:\n",
    "    # muestrear directamente dentro de bins_small\n",
    "    # para repartir mejor, distribuimos cuotas equitativas por bin\n",
    "    bins_ord = sorted(dfe_prior[\"time_bin\"].unique())\n",
    "    cuotas = cuotas_por_bin(faltan, len(bins_ord))\n",
    "    trozos = []\n",
    "    for bin_val, cuota in zip(bins_ord, cuotas):\n",
    "      cand = dfe_prior[dfe_prior[\"time_bin\"] == bin_val]\n",
    "      if cuota <= 0 or len(cand) == 0:\n",
    "        continue\n",
    "      if len(cand) <= cuota:\n",
    "        trozos.append(cand)\n",
    "      else:\n",
    "        trozos.append(cand.sample(n=cuota, random_state=rng))\n",
    "    sel_prior = pd.concat(trozos) if len(trozos) else dfe_prior.iloc[0:0]\n",
    "    # si por redondeos quedo corto, completar dentro de dfe_prior\n",
    "    if len(sel_prior) < faltan:\n",
    "      faltan2 = faltan - len(sel_prior)\n",
    "      cand_extra = dfe_prior.drop(sel_prior.index)\n",
    "      if len(cand_extra) > 0:\n",
    "        extra = cand_extra.sample(n=min(faltan2, len(cand_extra)), random_state=rng)\n",
    "        sel_prior = pd.concat([sel_prior, extra])\n",
    "    sel_prior = sel_prior.head(faltan)\n",
    "    selecciones[est] = pd.concat([sel_existente, sel_prior])\n",
    "  else:\n",
    "    # tomar todo lo que haya en bins_small y luego completar del resto\n",
    "    sel_prior = dfe_prior\n",
    "    faltan2 = faltan - len(sel_prior)\n",
    "    dfe_rest_extra = dfe_restante.drop(sel_prior.index)\n",
    "    if len(dfe_rest_extra) > 0:\n",
    "      if len(dfe_rest_extra) <= faltan2:\n",
    "        sel_extra = dfe_rest_extra\n",
    "      else:\n",
    "        sel_extra = dfe_rest_extra.sample(n=faltan2, random_state=rng)\n",
    "    else:\n",
    "      sel_extra = dfe_rest_extra.iloc[0:0]\n",
    "    selecciones[est] = pd.concat([sel_existente, sel_prior, sel_extra]).head(objetivo)\n",
    "\n",
    "# 9) Si PORT/UVER no alcanzan 200, completar sus faltantes repartiendo entre grandes\n",
    "grandes = [e for e in ESTACIONES_OBJ if e not in SMALL_STATIONS]\n",
    "pool_grandes = pd.concat([by_est[g].drop(selecciones[g].index) for g in grandes]).copy()\n",
    "\n",
    "for est in SMALL_STATIONS:\n",
    "  objetivo = TARGET_POR_EST\n",
    "  sel_est = selecciones[est]\n",
    "  faltan = max(0, objetivo - len(sel_est))\n",
    "  if faltan == 0:\n",
    "    continue\n",
    "\n",
    "  # priorizar completar desde bins_small (ya definido por pequenas)\n",
    "  pool_prior = pool_grandes[pool_grandes[\"time_bin\"].isin(bins_small)]\n",
    "  tomar = min(faltan, len(pool_prior))\n",
    "  if tomar > 0:\n",
    "    add = pool_prior.sample(n=tomar, random_state=rng)\n",
    "    selecciones[est] = pd.concat([sel_est, add])\n",
    "    pool_grandes = pool_grandes.drop(add.index)\n",
    "    faltan -= tomar\n",
    "\n",
    "  # si aun faltan, completar del pool restante\n",
    "  if faltan > 0 and len(pool_grandes) > 0:\n",
    "    add2 = pool_grandes.sample(n=min(faltan, len(pool_grandes)), random_state=rng)\n",
    "    selecciones[est] = pd.concat([selecciones[est], add2])\n",
    "    pool_grandes = pool_grandes.drop(add2.index)\n",
    "\n",
    "# 10) Ensamblar dataset final\n",
    "df_sampled = pd.concat([selecciones[e] for e in ESTACIONES_OBJ], ignore_index=True)\n",
    "\n",
    "# 11) Validaciones basicas\n",
    "print(\"Conteo por estacion (esperado hasta 200 c/u, PORT/UVER pueden incluir completados de otras):\")\n",
    "print(df_sampled[\"Estacion\"].value_counts())\n",
    "\n",
    "print(\"\\nMuestreo unico esperado 64.0 ->\", df_sampled[\"Muestreo\"].unique())\n",
    "na_tp = df_sampled[\"Pond T-P\"].isna().sum()\n",
    "na_ts = df_sampled[\"Pond T-S\"].isna().sum()\n",
    "print(f\"NA en Pond T-P: {na_tp} | NA en Pond T-S: {na_ts}\")\n",
    "\n",
    "# 12) Guardar\n",
    "df_sampled.to_csv(OUT_CSV, index=False)\n",
    "print(\"\\nGuardado en:\", OUT_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zgiIvHLi-3fb"
   },
   "source": [
    "**Nueva version que controla que T-P, T-S > T_ini**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3552,
     "status": "ok",
     "timestamp": 1757718346323,
     "user": {
      "displayName": "Rodrigo Muñoz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "LJlz0-PEHgGb",
    "outputId": "9fe1a811-fbdd-454e-cbcb-6280046de222"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ========= Rutas en Colab =========\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "IN_CSV  = '/content/drive/MyDrive/Colab Notebooks/TFM/test.csv'\n",
    "OUT_CSV = '/content/drive/MyDrive/Colab Notebooks/TFM/dataset_estratificado_1000.csv'\n",
    "# ==================================\n",
    "\n",
    "# ========= Parametros =========\n",
    "FREQ = \"W\"                 # \"D\" dia, \"W\" semana, \"M\" mes\n",
    "SEED = 42\n",
    "TARGET_POR_EST = 200\n",
    "ESTACIONES_OBJ = [\"LABR\", \"CUSH\", \"CHAI\", \"PORT\", \"UVER\"]\n",
    "SMALL_STATIONS = {\"PORT\", \"UVER\"}\n",
    "# ==============================\n",
    "\n",
    "rng = np.random.RandomState(SEED)\n",
    "\n",
    "# 1) Cargar y preparar\n",
    "df = pd.read_csv(IN_CSV, sep=\";\")\n",
    "\n",
    "# Estacion\n",
    "df.insert(0, \"Estacion\", df[\"mseed\"].str.split(\"_\").str[0])\n",
    "\n",
    "# Tiempos - convertir todas las columnas temporales\n",
    "df[\"T-ini\"] = pd.to_datetime(df[\"T-ini\"], errors=\"coerce\", utc=True)\n",
    "df[\"T-fin\"] = pd.to_datetime(df[\"T-fin\"], errors=\"coerce\", utc=True)\n",
    "df[\"T-P\"] = pd.to_datetime(df[\"T-P\"], errors=\"coerce\", utc=True)\n",
    "df[\"T-S\"] = pd.to_datetime(df[\"T-S\"], errors=\"coerce\", utc=True)\n",
    "\n",
    "# 2) Filtros estrictos requeridos\n",
    "df = df[df[\"Muestreo\"] == 64.0].copy()\n",
    "\n",
    "# Sin NA en ponderaciones: ambas deben existir\n",
    "df = df.dropna(subset=[\"Pond T-P\", \"Pond T-S\"], how=\"any\").copy()\n",
    "\n",
    "# NUEVO FILTRO: T-P > T-ini y T-S > T-ini\n",
    "print(f\"Registros antes del filtro temporal: {len(df)}\")\n",
    "df = df[(df[\"T-P\"] > df[\"T-ini\"]) & (df[\"T-S\"] > df[\"T-ini\"])].copy()\n",
    "print(f\"Registros después del filtro temporal (T-P>T-ini y T-S>T-ini): {len(df)}\")\n",
    "\n",
    "# 3) Mantener solo estaciones objetivo\n",
    "df = df[df[\"Estacion\"].isin(ESTACIONES_OBJ)].copy()\n",
    "print(f\"Registros después de filtro por estaciones objetivo: {len(df)}\")\n",
    "\n",
    "# 4) Bins temporales\n",
    "df[\"time_bin\"] = df[\"T-ini\"].dt.to_period(FREQ).dt.start_time\n",
    "\n",
    "# 5) Separar por estacion\n",
    "by_est = {est: df[df[\"Estacion\"] == est].copy() for est in ESTACIONES_OBJ}\n",
    "\n",
    "# Mostrar disponibilidad por estación después de todos los filtros\n",
    "print(\"\\nRegistros disponibles por estación después de filtros:\")\n",
    "for est in ESTACIONES_OBJ:\n",
    "    print(f\"{est}: {len(by_est[est])} registros\")\n",
    "\n",
    "# 6) Seleccion para estaciones pequenas: sin criterio temporal, tomar todo hasta TARGET\n",
    "selecciones = {}\n",
    "for est in ESTACIONES_OBJ:\n",
    "  dfe = by_est[est]\n",
    "  if est in SMALL_STATIONS:\n",
    "    if len(dfe) <= TARGET_POR_EST:\n",
    "      selecciones[est] = dfe.copy()\n",
    "    else:\n",
    "      selecciones[est] = dfe.sample(n=TARGET_POR_EST, random_state=rng)  # por si hay mas de 200\n",
    "  else:\n",
    "    selecciones[est] = dfe.iloc[0:0].copy()  # placeholder para grandes\n",
    "\n",
    "# 7) Construir el conjunto de bins a priorizar según PORT y UVER (fechas similares)\n",
    "bins_small = set(pd.concat([selecciones[s] for s in SMALL_STATIONS if s in selecciones and len(selecciones[s]) > 0])[\"time_bin\"].unique())\n",
    "bins_small = sorted([b for b in bins_small if pd.notna(b)])\n",
    "\n",
    "# Si no hay bins en las pequenas (muy extremo), usar los bins mas frecuentes del conjunto total\n",
    "if len(bins_small) == 0:\n",
    "  bins_small = list(df[\"time_bin\"].value_counts().index[:10])  # top 10 como fallback\n",
    "\n",
    "# 8) Asignar cupos a estaciones grandes (LABR, CUSH, CHAI)\n",
    "def cuotas_por_bin(total_objetivo, num_bins):\n",
    "  base = total_objetivo // num_bins if num_bins > 0 else 0\n",
    "  resto = total_objetivo % num_bins if num_bins > 0 else 0\n",
    "  cuotas = np.full(max(num_bins,1), base, dtype=int)\n",
    "  cuotas[:resto] += 1\n",
    "  return cuotas\n",
    "\n",
    "for est in ESTACIONES_OBJ:\n",
    "  if est in SMALL_STATIONS:\n",
    "    continue\n",
    "  dfe = by_est[est].copy()\n",
    "  # objetivo por estacion\n",
    "  objetivo = TARGET_POR_EST\n",
    "\n",
    "  # ya hay seleccion para est? (deberia estar vacia para grandes)\n",
    "  sel_existente = selecciones.get(est, dfe.iloc[0:0].copy())\n",
    "\n",
    "  faltan = max(0, objetivo - len(sel_existente))\n",
    "  if faltan == 0:\n",
    "    continue\n",
    "\n",
    "  # 8a) Intentar cubrir faltantes priorizando bins_small\n",
    "  dfe_restante = dfe.drop(sel_existente.index)\n",
    "  dfe_prior = dfe_restante[dfe_restante[\"time_bin\"].isin(bins_small)].copy()\n",
    "\n",
    "  if len(dfe_prior) >= faltan:\n",
    "    # muestrear directamente dentro de bins_small\n",
    "    # para repartir mejor, distribuimos cuotas equitativas por bin\n",
    "    bins_ord = sorted(dfe_prior[\"time_bin\"].unique())\n",
    "    cuotas = cuotas_por_bin(faltan, len(bins_ord))\n",
    "    trozos = []\n",
    "    for bin_val, cuota in zip(bins_ord, cuotas):\n",
    "      cand = dfe_prior[dfe_prior[\"time_bin\"] == bin_val]\n",
    "      if cuota <= 0 or len(cand) == 0:\n",
    "        continue\n",
    "      if len(cand) <= cuota:\n",
    "        trozos.append(cand)\n",
    "      else:\n",
    "        trozos.append(cand.sample(n=cuota, random_state=rng))\n",
    "    sel_prior = pd.concat(trozos) if len(trozos) else dfe_prior.iloc[0:0]\n",
    "    # si por redondeos quedo corto, completar dentro de dfe_prior\n",
    "    if len(sel_prior) < faltan:\n",
    "      faltan2 = faltan - len(sel_prior)\n",
    "      cand_extra = dfe_prior.drop(sel_prior.index)\n",
    "      if len(cand_extra) > 0:\n",
    "        extra = cand_extra.sample(n=min(faltan2, len(cand_extra)), random_state=rng)\n",
    "        sel_prior = pd.concat([sel_prior, extra])\n",
    "    sel_prior = sel_prior.head(faltan)\n",
    "    selecciones[est] = pd.concat([sel_existente, sel_prior])\n",
    "  else:\n",
    "    # tomar todo lo que haya en bins_small y luego completar del resto\n",
    "    sel_prior = dfe_prior\n",
    "    faltan2 = faltan - len(sel_prior)\n",
    "    dfe_rest_extra = dfe_restante.drop(sel_prior.index)\n",
    "    if len(dfe_rest_extra) > 0:\n",
    "      if len(dfe_rest_extra) <= faltan2:\n",
    "        sel_extra = dfe_rest_extra\n",
    "      else:\n",
    "        sel_extra = dfe_rest_extra.sample(n=faltan2, random_state=rng)\n",
    "    else:\n",
    "      sel_extra = dfe_rest_extra.iloc[0:0]\n",
    "    selecciones[est] = pd.concat([sel_existente, sel_prior, sel_extra]).head(objetivo)\n",
    "\n",
    "# 9) Si PORT/UVER no alcanzan 200, completar sus faltantes repartiendo entre grandes\n",
    "grandes = [e for e in ESTACIONES_OBJ if e not in SMALL_STATIONS]\n",
    "pool_grandes = pd.concat([by_est[g].drop(selecciones[g].index) for g in grandes]).copy()\n",
    "\n",
    "for est in SMALL_STATIONS:\n",
    "  objetivo = TARGET_POR_EST\n",
    "  sel_est = selecciones[est]\n",
    "  faltan = max(0, objetivo - len(sel_est))\n",
    "  if faltan == 0:\n",
    "    continue\n",
    "\n",
    "  # priorizar completar desde bins_small (ya definido por pequenas)\n",
    "  pool_prior = pool_grandes[pool_grandes[\"time_bin\"].isin(bins_small)]\n",
    "  tomar = min(faltan, len(pool_prior))\n",
    "  if tomar > 0:\n",
    "    add = pool_prior.sample(n=tomar, random_state=rng)\n",
    "    selecciones[est] = pd.concat([sel_est, add])\n",
    "    pool_grandes = pool_grandes.drop(add.index)\n",
    "    faltan -= tomar\n",
    "\n",
    "  # si aun faltan, completar del pool restante\n",
    "  if faltan > 0 and len(pool_grandes) > 0:\n",
    "    add2 = pool_grandes.sample(n=min(faltan, len(pool_grandes)), random_state=rng)\n",
    "    selecciones[est] = pd.concat([selecciones[est], add2])\n",
    "    pool_grandes = pool_grandes.drop(add2.index)\n",
    "\n",
    "# 10) Ensamblar dataset final\n",
    "df_sampled = pd.concat([selecciones[e] for e in ESTACIONES_OBJ], ignore_index=True)\n",
    "\n",
    "# 11) Validaciones basicas\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTADOS FINALES\")\n",
    "print(\"=\"*50)\n",
    "print(\"Conteo por estacion (esperado hasta 20 c/u, PORT/UVER pueden incluir completados de otras):\")\n",
    "print(df_sampled[\"Estacion\"].value_counts())\n",
    "\n",
    "print(f\"\\nMuestreo unico esperado 64.0 -> {df_sampled['Muestreo'].unique()}\")\n",
    "na_tp = df_sampled[\"Pond T-P\"].isna().sum()\n",
    "na_ts = df_sampled[\"Pond T-S\"].isna().sum()\n",
    "print(f\"NA en Pond T-P: {na_tp} | NA en Pond T-S: {na_ts}\")\n",
    "\n",
    "# Validación adicional del filtro temporal aplicado\n",
    "violaciones_tp = (df_sampled[\"T-P\"] <= df_sampled[\"T-ini\"]).sum()\n",
    "violaciones_ts = (df_sampled[\"T-S\"] <= df_sampled[\"T-ini\"]).sum()\n",
    "print(f\"\\nValidación filtro temporal:\")\n",
    "print(f\"Violaciones T-P <= T-ini: {violaciones_tp} (debe ser 0)\")\n",
    "print(f\"Violaciones T-S <= T-ini: {violaciones_ts} (debe ser 0)\")\n",
    "\n",
    "# 12) Formatear tiempos antes de guardar\n",
    "def format_datetime_iso(dt_series):\n",
    "    \"\"\"Convierte datetime a formato ISO 8601 con milisegundos y zona UTC\"\"\"\n",
    "    return dt_series.dt.strftime('%Y-%m-%dT%H:%M:%S.%f').str[:-3] + 'Z'\n",
    "\n",
    "# Crear copia para formateo sin afectar los datos originales\n",
    "df_output = df_sampled.copy()\n",
    "\n",
    "# Formatear todas las columnas de tiempo\n",
    "time_columns = ['T-ini', 'T-fin', 'T-P', 'T-S']\n",
    "for col in time_columns:\n",
    "    if col in df_output.columns:\n",
    "        df_output[col] = format_datetime_iso(df_output[col])\n",
    "\n",
    "print(f\"\\nEjemplo de formato de tiempos en salida:\")\n",
    "print(f\"T-ini: {df_output['T-ini'].iloc[0]}\")\n",
    "print(f\"T-P: {df_output['T-P'].iloc[0]}\")\n",
    "print(f\"T-S: {df_output['T-S'].iloc[0]}\")\n",
    "\n",
    "# Guardar con formato preservado\n",
    "df_output.to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nGuardado en: {OUT_CSV}\")\n",
    "print(f\"Total de registros en dataset final: {len(df_sampled)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNwyFLlBXB6rKOFZwuGNeZA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jup_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
