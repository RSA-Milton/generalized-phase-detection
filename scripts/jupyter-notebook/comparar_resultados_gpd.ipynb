{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsYHAyOeFIF_"
   },
   "source": [
    "**Paso 1: Configuraci√≥n inicial y carga de datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n inicial\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1114,
     "status": "ok",
     "timestamp": 1758209867325,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "mwcsggydDzlt",
    "outputId": "7161b0fc-92ae-452c-9ca2-201193ad2cab"
   },
   "outputs": [],
   "source": [
    "# Configurar estilo de gr√°ficas\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Cargar datos - ajusta las rutas seg√∫n tu estructura\n",
    "archivo_referencia = '/content/drive/MyDrive/Colab Notebooks/TFM/referencia_analista.csv'\n",
    "archivo_gpd = '/content/drive/MyDrive/Colab Notebooks/TFM/resultados_gpd.csv'\n",
    "\n",
    "# Cargar datasets\n",
    "print(\"Cargando datos...\")\n",
    "df_referencia = pd.read_csv(archivo_referencia)\n",
    "df_gpd = pd.read_csv(archivo_gpd)\n",
    "\n",
    "print(f\"‚úì Datos de referencia: {len(df_referencia)} eventos\")\n",
    "print(f\"‚úì Datos GPD: {len(df_gpd)} eventos\")\n",
    "print(f\"‚úì Columnas referencia: {list(df_referencia.columns)}\")\n",
    "print(f\"‚úì Columnas GPD: {list(df_gpd.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ud5a5y87FLRx"
   },
   "source": [
    "**Paso 2: Exploraci√≥n inicial de los datos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1758209867326,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "pZQchBmiFFWc",
    "outputId": "e70a2969-8295-4d2c-9c1a-114fa95202f4"
   },
   "outputs": [],
   "source": [
    "# Exploraci√≥n b√°sica de los datos de referencia\n",
    "print(\"=== EXPLORACI√ìN DATOS DE REFERENCIA ===\")\n",
    "print(f\"Forma del dataset: {df_referencia.shape}\")\n",
    "print(f\"Estaciones √∫nicas: {sorted(df_referencia['Estacion'].unique())}\")\n",
    "print(f\"Rango de fechas T-ini: {df_referencia['T-ini'].min()} a {df_referencia['T-ini'].max()}\")\n",
    "\n",
    "# Estad√≠sticas b√°sicas de SNR\n",
    "print(\"\\n=== ESTAD√çSTICAS SNR ===\")\n",
    "print(\"SNR-P:\")\n",
    "print(df_referencia['SNR-P'].describe())\n",
    "print(\"\\nSNR-S:\")\n",
    "print(df_referencia['SNR-S'].describe())\n",
    "\n",
    "# Verificar valores nulos en columnas clave\n",
    "print(\"\\n=== VALORES NULOS ===\")\n",
    "columnas_clave = ['T-P', 'T-S', 'SNR-P', 'SNR-S', 'Pond T-P', 'Pond T-S']\n",
    "for col in columnas_clave:\n",
    "    nulos = df_referencia[col].isnull().sum()\n",
    "    print(f\"{col}: {nulos} nulos ({nulos/len(df_referencia)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nQfkVOpFY_G"
   },
   "source": [
    "**Paso 3: Distribuciones de SNR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1924,
     "status": "ok",
     "timestamp": 1758209869250,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "iHCXU7qXFZS2",
    "outputId": "d0bd9493-a9e8-4c54-d227-4b4853e200b5"
   },
   "outputs": [],
   "source": [
    "# Visualizar distribuciones de SNR\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Histograma SNR-P\n",
    "axes[0,0].hist(df_referencia['SNR-P'].dropna(), bins=50, alpha=0.7, color='blue', edgecolor='black')\n",
    "axes[0,0].set_xlabel('SNR-P (dB)')\n",
    "axes[0,0].set_ylabel('Frecuencia')\n",
    "axes[0,0].set_title('Distribuci√≥n SNR Fase P')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Histograma SNR-S\n",
    "axes[0,1].hist(df_referencia['SNR-S'].dropna(), bins=50, alpha=0.7, color='red', edgecolor='black')\n",
    "axes[0,1].set_xlabel('SNR-S (dB)')\n",
    "axes[0,1].set_ylabel('Frecuencia')\n",
    "axes[0,1].set_title('Distribuci√≥n SNR Fase S')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot comparativo\n",
    "data_box = [df_referencia['SNR-P'].dropna(), df_referencia['SNR-S'].dropna()]\n",
    "axes[1,0].boxplot(data_box, labels=['SNR-P', 'SNR-S'])\n",
    "axes[1,0].set_ylabel('SNR (dB)')\n",
    "axes[1,0].set_title('Comparaci√≥n SNR P vs S')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot SNR-P vs SNR-S\n",
    "valid_snr = df_referencia.dropna(subset=['SNR-P', 'SNR-S'])\n",
    "axes[1,1].scatter(valid_snr['SNR-P'], valid_snr['SNR-S'], alpha=0.6, s=20)\n",
    "axes[1,1].set_xlabel('SNR-P (dB)')\n",
    "axes[1,1].set_ylabel('SNR-S (dB)')\n",
    "axes[1,1].set_title('SNR-P vs SNR-S')\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# L√≠nea de igualdad\n",
    "min_snr = min(valid_snr['SNR-P'].min(), valid_snr['SNR-S'].min())\n",
    "max_snr = max(valid_snr['SNR-P'].max(), valid_snr['SNR-S'].max())\n",
    "axes[1,1].plot([min_snr, max_snr], [min_snr, max_snr], 'r--', alpha=0.7, label='SNR-P = SNR-S')\n",
    "axes[1,1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estad√≠sticas de correlaci√≥n\n",
    "correlacion = valid_snr['SNR-P'].corr(valid_snr['SNR-S'])\n",
    "print(f\"\\nCorrelaci√≥n SNR-P vs SNR-S: {correlacion:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pV05bUHiFtv7"
   },
   "source": [
    "**Paso 4: Merge de datos y verificaci√≥n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1758209869267,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "FesnXRBxFxPq",
    "outputId": "53c48ef1-1b17-4545-c39a-beb2d1400c51"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para parsear timestamps (adaptada del script original)\n",
    "def parse_timestamp(ts_str):\n",
    "    \"\"\"Convierte timestamp a datetime con manejo robusto de formatos\"\"\"\n",
    "    if pd.isna(ts_str) or ts_str == 'NA':\n",
    "        return None\n",
    "    try:\n",
    "        ts_str = str(ts_str).replace('Z', '+00:00')\n",
    "        if '+' not in ts_str and 'T' in ts_str:\n",
    "            ts_str += '+00:00'\n",
    "        return pd.to_datetime(ts_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Merge de datos\n",
    "print(\"Realizando merge de datos...\")\n",
    "df_combined = pd.merge(\n",
    "    df_referencia[['Estacion', 'mseed', 'T-P', 'T-S', 'Pond T-P', 'Pond T-S', 'SNR-P', 'SNR-S']],\n",
    "    df_gpd[['Estacion', 'mseed', 'T-P', 'T-S', 'Pond T-P', 'Pond T-S']],\n",
    "    on=['Estacion', 'mseed'],\n",
    "    how='inner',\n",
    "    suffixes=('_ref', '_gpd')\n",
    ")\n",
    "\n",
    "print(f\"‚úì Eventos coincidentes: {len(df_combined)}\")\n",
    "print(f\"‚úì Estaciones en datos combinados: {sorted(df_combined['Estacion'].unique())}\")\n",
    "\n",
    "# Verificar el merge\n",
    "total_ref = len(df_referencia)\n",
    "total_gpd = len(df_gpd)\n",
    "total_merged = len(df_combined)\n",
    "\n",
    "print(f\"\\nEficiencia del merge:\")\n",
    "print(f\"- Eventos referencia utilizados: {total_merged}/{total_ref} ({total_merged/total_ref*100:.1f}%)\")\n",
    "print(f\"- Eventos GPD utilizados: {total_merged}/{total_gpd} ({total_merged/total_gpd*100:.1f}%)\")\n",
    "\n",
    "# Mostrar primeras filas para verificaci√≥n\n",
    "print(f\"\\nPrimeras 3 filas del dataset combinado:\")\n",
    "print(df_combined[['Estacion', 'mseed', 'SNR-P', 'SNR-S']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPEQgbTXF4im"
   },
   "source": [
    "**Paso 5: Distribuci√≥n de SNR por estaci√≥n**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758209869279,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "Gmp9s25qGOQt",
    "outputId": "5539bc4c-a855-4066-debb-c44f94f3fa6e"
   },
   "outputs": [],
   "source": [
    "# An√°lisis de distribuci√≥n de SNR por estaci√≥n\n",
    "print(\"=== AN√ÅLISIS SNR POR ESTACI√ìN ===\")\n",
    "\n",
    "estaciones = sorted(df_combined['Estacion'].unique())\n",
    "print(f\"Estaciones disponibles: {estaciones}\")\n",
    "print(f\"N√∫mero de estaciones: {len(estaciones)}\")\n",
    "\n",
    "# Estad√≠sticas b√°sicas por estaci√≥n\n",
    "print(\"\\n=== ESTAD√çSTICAS DESCRIPTIVAS POR ESTACI√ìN ===\")\n",
    "stats_por_estacion = df_combined.groupby('Estacion')[['SNR-P', 'SNR-S']].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(2)\n",
    "\n",
    "print(stats_por_estacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 911,
     "status": "ok",
     "timestamp": 1758209870191,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "iruKgeInGYCL",
    "outputId": "e725569d-1d58-4838-b4f0-fdc175a0c479"
   },
   "outputs": [],
   "source": [
    "# Gr√°fica 1: Box plots por estaci√≥n\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Box plot SNR-P por estaci√≥n\n",
    "df_combined.boxplot(column='SNR-P', by='Estacion', ax=axes[0])\n",
    "axes[0].set_title('Distribuci√≥n SNR-P por Estaci√≥n')\n",
    "axes[0].set_xlabel('Estaci√≥n')\n",
    "axes[0].set_ylabel('SNR-P (dB)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Box plot SNR-S por estaci√≥n\n",
    "df_combined.boxplot(column='SNR-S', by='Estacion', ax=axes[1])\n",
    "axes[1].set_title('Distribuci√≥n SNR-S por Estaci√≥n')\n",
    "axes[1].set_xlabel('Estaci√≥n')\n",
    "axes[1].set_ylabel('SNR-S (dB)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 794
    },
    "executionInfo": {
     "elapsed": 812,
     "status": "ok",
     "timestamp": 1758209871004,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "y4JuineUGeMa",
    "outputId": "8673a2e3-7077-4527-bba0-e1f99ca7929b"
   },
   "outputs": [],
   "source": [
    "# Gr√°fica 2: Violin plots m√°s detallados\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Violin plot SNR-P\n",
    "sns.violinplot(data=df_combined, x='Estacion', y='SNR-P', ax=axes[0])\n",
    "axes[0].set_title('Distribuci√≥n Detallada SNR-P por Estaci√≥n')\n",
    "axes[0].set_xlabel('Estaci√≥n')\n",
    "axes[0].set_ylabel('SNR-P (dB)')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Violin plot SNR-S\n",
    "sns.violinplot(data=df_combined, x='Estacion', y='SNR-S', ax=axes[1])\n",
    "axes[1].set_title('Distribuci√≥n Detallada SNR-S por Estaci√≥n')\n",
    "axes[1].set_xlabel('Estaci√≥n')\n",
    "axes[1].set_ylabel('SNR-S (dB)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 473,
     "status": "ok",
     "timestamp": 1758209871488,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "5mjRYCEZGk2r",
    "outputId": "947083ed-d653-485f-e490-c30446d6c3bb"
   },
   "outputs": [],
   "source": [
    "# Gr√°fica 3: Heatmap comparativo\n",
    "# Crear matriz de estad√≠sticas para visualizaci√≥n\n",
    "stats_matrix_p = df_combined.groupby('Estacion')['SNR-P'].agg(['mean', 'median', 'std']).round(1)\n",
    "stats_matrix_s = df_combined.groupby('Estacion')['SNR-S'].agg(['mean', 'median', 'std']).round(1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Heatmap SNR-P\n",
    "sns.heatmap(stats_matrix_p.T, annot=True, cmap='RdYlBu_r', ax=axes[0],\n",
    "            cbar_kws={'label': 'SNR-P (dB)'})\n",
    "axes[0].set_title('Estad√≠sticas SNR-P por Estaci√≥n')\n",
    "axes[0].set_xlabel('Estaci√≥n')\n",
    "axes[0].set_ylabel('Estad√≠stica')\n",
    "\n",
    "# Heatmap SNR-S\n",
    "sns.heatmap(stats_matrix_s.T, annot=True, cmap='RdYlBu_r', ax=axes[1],\n",
    "            cbar_kws={'label': 'SNR-S (dB)'})\n",
    "axes[1].set_title('Estad√≠sticas SNR-S por Estaci√≥n')\n",
    "axes[1].set_xlabel('Estaci√≥n')\n",
    "axes[1].set_ylabel('Estad√≠stica')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758209871492,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "sAl5JLMKGxMx",
    "outputId": "27004f00-b641-4d49-c82a-d552c27f245a"
   },
   "outputs": [],
   "source": [
    "# An√°lisis estad√≠stico: Test ANOVA para diferencias significativas\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\n=== AN√ÅLISIS ESTAD√çSTICO ===\")\n",
    "\n",
    "# Preparar datos por estaci√≥n para ANOVA\n",
    "grupos_snr_p = [df_combined[df_combined['Estacion'] == est]['SNR-P'].dropna()\n",
    "                for est in estaciones]\n",
    "grupos_snr_s = [df_combined[df_combined['Estacion'] == est]['SNR-S'].dropna()\n",
    "                for est in estaciones]\n",
    "\n",
    "# Test ANOVA\n",
    "f_stat_p, p_value_p = stats.f_oneway(*grupos_snr_p)\n",
    "f_stat_s, p_value_s = stats.f_oneway(*grupos_snr_s)\n",
    "\n",
    "print(f\"ANOVA SNR-P entre estaciones:\")\n",
    "print(f\"  F-statistic: {f_stat_p:.3f}\")\n",
    "print(f\"  p-value: {p_value_p:.6f}\")\n",
    "print(f\"  Diferencias significativas: {'S√ç' if p_value_p < 0.05 else 'NO'}\")\n",
    "\n",
    "print(f\"\\nANOVA SNR-S entre estaciones:\")\n",
    "print(f\"  F-statistic: {f_stat_s:.3f}\")\n",
    "print(f\"  p-value: {p_value_s:.6f}\")\n",
    "print(f\"  Diferencias significativas: {'S√ç' if p_value_s < 0.05 else 'NO'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "executionInfo": {
     "elapsed": 446,
     "status": "ok",
     "timestamp": 1758209871938,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "uiCORif7G1Zm",
    "outputId": "3eab75ee-84cd-48fd-cabc-ea764fe352ee"
   },
   "outputs": [],
   "source": [
    "# Gr√°fica 4: Scatter plot SNR-P vs SNR-S coloreado por estaci√≥n\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(estaciones)))\n",
    "for i, estacion in enumerate(estaciones):\n",
    "    data_est = df_combined[df_combined['Estacion'] == estacion]\n",
    "    plt.scatter(data_est['SNR-P'], data_est['SNR-S'],\n",
    "               label=estacion, alpha=0.7, s=30, color=colors[i])\n",
    "\n",
    "plt.xlabel('SNR-P (dB)')\n",
    "plt.ylabel('SNR-S (dB)')\n",
    "plt.title('SNR-P vs SNR-S por Estaci√≥n')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# L√≠nea de igualdad\n",
    "min_snr = min(df_combined['SNR-P'].min(), df_combined['SNR-S'].min())\n",
    "max_snr = max(df_combined['SNR-P'].max(), df_combined['SNR-S'].max())\n",
    "plt.plot([min_snr, max_snr], [min_snr, max_snr], 'k--', alpha=0.5, label='SNR-P = SNR-S')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758209871944,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "nSo1lbWaG50r",
    "outputId": "8ce4382c-7992-4019-f605-4627f1248b39"
   },
   "outputs": [],
   "source": [
    "# Tabla resumen: Ranking de estaciones por SNR\n",
    "print(\"\\n=== RANKING DE ESTACIONES POR SNR ===\")\n",
    "\n",
    "ranking_data = []\n",
    "for estacion in estaciones:\n",
    "    data_est = df_combined[df_combined['Estacion'] == estacion]\n",
    "    ranking_data.append({\n",
    "        'Estacion': estacion,\n",
    "        'N_eventos': len(data_est),\n",
    "        'SNR-P_medio': data_est['SNR-P'].mean(),\n",
    "        'SNR-S_medio': data_est['SNR-S'].mean(),\n",
    "        'SNR-P_mediano': data_est['SNR-P'].median(),\n",
    "        'SNR-S_mediano': data_est['SNR-S'].median(),\n",
    "        'SNR-P_std': data_est['SNR-P'].std(),\n",
    "        'SNR-S_std': data_est['SNR-S'].std()\n",
    "    })\n",
    "\n",
    "df_ranking = pd.DataFrame(ranking_data)\n",
    "\n",
    "# Ordenar por SNR-P medio descendente\n",
    "df_ranking_p = df_ranking.sort_values('SNR-P_medio', ascending=False)\n",
    "print(\"Ranking por SNR-P medio:\")\n",
    "print(df_ranking_p[['Estacion', 'N_eventos', 'SNR-P_medio', 'SNR-P_mediano', 'SNR-P_std']].round(2))\n",
    "\n",
    "print(f\"\\nRanking por SNR-S medio:\")\n",
    "df_ranking_s = df_ranking.sort_values('SNR-S_medio', ascending=False)\n",
    "print(df_ranking_s[['Estacion', 'N_eventos', 'SNR-S_medio', 'SNR-S_mediano', 'SNR-S_std']].round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAMEoJAfHK3O"
   },
   "source": [
    "**Paso 6: Identificaci√≥n de estaciones problem√°ticas vs robustas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758209871949,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "F822ciMfHUC5",
    "outputId": "b2316c22-5685-4964-c163-f2f98010feeb"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para calcular m√©tricas de robustez por estaci√≥n\n",
    "def calcular_metricas_robustez(df_combined):\n",
    "    \"\"\"Calcula m√©tricas que indican robustez de cada estaci√≥n\"\"\"\n",
    "\n",
    "    metricas_estacion = []\n",
    "\n",
    "    for estacion in sorted(df_combined['Estacion'].unique()):\n",
    "        data_est = df_combined[df_combined['Estacion'] == estacion].copy()\n",
    "\n",
    "        # M√©tricas b√°sicas\n",
    "        n_eventos = len(data_est)\n",
    "\n",
    "        # M√©tricas de SNR\n",
    "        snr_p_stats = {\n",
    "            'snr_p_mean': data_est['SNR-P'].mean(),\n",
    "            'snr_p_median': data_est['SNR-P'].median(),\n",
    "            'snr_p_std': data_est['SNR-P'].std(),\n",
    "            'snr_p_min': data_est['SNR-P'].min(),\n",
    "            'snr_p_q25': data_est['SNR-P'].quantile(0.25),\n",
    "            'snr_p_q75': data_est['SNR-P'].quantile(0.75)\n",
    "        }\n",
    "\n",
    "        snr_s_stats = {\n",
    "            'snr_s_mean': data_est['SNR-S'].mean(),\n",
    "            'snr_s_median': data_est['SNR-S'].median(),\n",
    "            'snr_s_std': data_est['SNR-S'].std(),\n",
    "            'snr_s_min': data_est['SNR-S'].min(),\n",
    "            'snr_s_q25': data_est['SNR-S'].quantile(0.25),\n",
    "            'snr_s_q75': data_est['SNR-S'].quantile(0.75)\n",
    "        }\n",
    "\n",
    "        # M√©tricas de robustez\n",
    "        # 1. Porcentaje de eventos con SNR bajo\n",
    "        eventos_snr_p_bajo = (data_est['SNR-P'] < 10).sum()  # <10 dB considerado bajo\n",
    "        eventos_snr_s_bajo = (data_est['SNR-S'] < 10).sum()\n",
    "\n",
    "        # 2. Consistencia (coeficiente de variaci√≥n)\n",
    "        cv_p = snr_p_stats['snr_p_std'] / snr_p_stats['snr_p_mean'] if snr_p_stats['snr_p_mean'] > 0 else np.inf\n",
    "        cv_s = snr_s_stats['snr_s_std'] / snr_s_stats['snr_s_mean'] if snr_s_stats['snr_s_mean'] > 0 else np.inf\n",
    "\n",
    "        # 3. Rango intercuart√≠lico (IQR) - menor IQR = m√°s consistente\n",
    "        iqr_p = snr_p_stats['snr_p_q75'] - snr_p_stats['snr_p_q25']\n",
    "        iqr_s = snr_s_stats['snr_s_q75'] - snr_s_stats['snr_s_q25']\n",
    "\n",
    "        # 4. Score de robustez compuesto (0-100, mayor = m√°s robusta)\n",
    "        # Basado en: SNR medio, consistencia, y porcentaje de eventos de calidad\n",
    "        score_snr_p = min(snr_p_stats['snr_p_mean'] / 20 * 40, 40)  # Max 40 puntos por SNR medio\n",
    "        score_snr_s = min(snr_s_stats['snr_s_mean'] / 20 * 40, 40)  # Max 40 puntos por SNR medio\n",
    "\n",
    "        score_consistencia_p = max(0, 20 - cv_p * 10)  # Max 20 puntos por consistencia\n",
    "        score_consistencia_s = max(0, 20 - cv_s * 10)  # Max 20 puntos por consistencia\n",
    "\n",
    "        score_calidad_p = (1 - eventos_snr_p_bajo/n_eventos) * 20  # Max 20 puntos por pocos eventos malos\n",
    "        score_calidad_s = (1 - eventos_snr_s_bajo/n_eventos) * 20  # Max 20 puntos por pocos eventos malos\n",
    "\n",
    "        robustez_score_p = score_snr_p + score_consistencia_p + score_calidad_p\n",
    "        robustez_score_s = score_snr_s + score_consistencia_s + score_calidad_s\n",
    "        robustez_score_total = (robustez_score_p + robustez_score_s) / 2\n",
    "\n",
    "        metricas_estacion.append({\n",
    "            'Estacion': estacion,\n",
    "            'N_eventos': n_eventos,\n",
    "            **snr_p_stats,\n",
    "            **snr_s_stats,\n",
    "            'eventos_snr_p_bajo': eventos_snr_p_bajo,\n",
    "            'eventos_snr_s_bajo': eventos_snr_s_bajo,\n",
    "            'pct_snr_p_bajo': eventos_snr_p_bajo/n_eventos*100,\n",
    "            'pct_snr_s_bajo': eventos_snr_s_bajo/n_eventos*100,\n",
    "            'cv_p': cv_p,\n",
    "            'cv_s': cv_s,\n",
    "            'iqr_p': iqr_p,\n",
    "            'iqr_s': iqr_s,\n",
    "            'robustez_score_p': robustez_score_p,\n",
    "            'robustez_score_s': robustez_score_s,\n",
    "            'robustez_score_total': robustez_score_total\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metricas_estacion)\n",
    "\n",
    "# Calcular m√©tricas\n",
    "print(\"=== AN√ÅLISIS DE ROBUSTEZ POR ESTACI√ìN ===\")\n",
    "df_robustez = calcular_metricas_robustez(df_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758209871955,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "97dw2NKDHX2T",
    "outputId": "5d5f1221-13f6-4566-bf37-40e8d00bffb5"
   },
   "outputs": [],
   "source": [
    "# Clasificar estaciones por robustez\n",
    "def clasificar_estaciones(df_robustez):\n",
    "    \"\"\"Clasifica estaciones en robustas, intermedias y problem√°ticas\"\"\"\n",
    "\n",
    "    # Usar percentiles para clasificaci√≥n\n",
    "    q33 = df_robustez['robustez_score_total'].quantile(0.33)\n",
    "    q67 = df_robustez['robustez_score_total'].quantile(0.67)\n",
    "\n",
    "    def clasificar(score):\n",
    "        if score >= q67:\n",
    "            return 'Robusta'\n",
    "        elif score >= q33:\n",
    "            return 'Intermedia'\n",
    "        else:\n",
    "            return 'Problem√°tica'\n",
    "\n",
    "    df_robustez['Clasificacion'] = df_robustez['robustez_score_total'].apply(clasificar)\n",
    "    return df_robustez, q33, q67\n",
    "\n",
    "df_robustez, umbral_prob, umbral_rob = clasificar_estaciones(df_robustez)\n",
    "\n",
    "# Mostrar clasificaci√≥n\n",
    "print(f\"Umbrales de clasificaci√≥n:\")\n",
    "print(f\"  Problem√°tica: Score < {umbral_prob:.1f}\")\n",
    "print(f\"  Intermedia: {umbral_prob:.1f} ‚â§ Score < {umbral_rob:.1f}\")\n",
    "print(f\"  Robusta: Score ‚â• {umbral_rob:.1f}\")\n",
    "\n",
    "print(f\"\\n=== CLASIFICACI√ìN DE ESTACIONES ===\")\n",
    "for categoria in ['Problem√°tica', 'Intermedia', 'Robusta']:\n",
    "    estaciones_cat = df_robustez[df_robustez['Clasificacion'] == categoria]['Estacion'].tolist()\n",
    "    print(f\"{categoria}: {estaciones_cat} ({len(estaciones_cat)} estaciones)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758209872038,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "XaEnP13sHfJj",
    "outputId": "6d0b7d4c-2da7-4ebc-bc74-8a1cd666b84a"
   },
   "outputs": [],
   "source": [
    "# Tabla resumen ordenada por robustez\n",
    "print(f\"\\n=== RANKING COMPLETO DE ROBUSTEZ ===\")\n",
    "df_ranking_robustez = df_robustez.sort_values('robustez_score_total', ascending=False)\n",
    "\n",
    "columnas_mostrar = ['Estacion', 'Clasificacion', 'N_eventos', 'robustez_score_total',\n",
    "                   'snr_p_mean', 'snr_s_mean', 'pct_snr_p_bajo', 'pct_snr_s_bajo',\n",
    "                   'cv_p', 'cv_s']\n",
    "\n",
    "print(df_ranking_robustez[columnas_mostrar].round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 809,
     "status": "ok",
     "timestamp": 1758209872847,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "9lGSbinzHmK4",
    "outputId": "2a72ebf8-05cc-4652-e6ee-59269b3444ce"
   },
   "outputs": [],
   "source": [
    "# Visualizaci√≥n 1: Gr√°fica de robustez general\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Scores de robustez por estaci√≥n\n",
    "estaciones_ord = df_robustez.sort_values('robustez_score_total')['Estacion']\n",
    "scores_ord = df_robustez.sort_values('robustez_score_total')['robustez_score_total']\n",
    "colores = ['red' if score < umbral_prob else 'orange' if score < umbral_rob else 'green'\n",
    "           for score in scores_ord]\n",
    "\n",
    "axes[0,0].barh(range(len(estaciones_ord)), scores_ord, color=colores, alpha=0.7)\n",
    "axes[0,0].set_yticks(range(len(estaciones_ord)))\n",
    "axes[0,0].set_yticklabels(estaciones_ord)\n",
    "axes[0,0].set_xlabel('Score de Robustez Total')\n",
    "axes[0,0].set_title('Ranking de Robustez por Estaci√≥n')\n",
    "axes[0,0].axvline(umbral_prob, color='red', linestyle='--', alpha=0.7, label='Umbral Problem√°tica')\n",
    "axes[0,0].axvline(umbral_rob, color='green', linestyle='--', alpha=0.7, label='Umbral Robusta')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. SNR medio vs Variabilidad\n",
    "axes[0,1].scatter(df_robustez['snr_p_mean'], df_robustez['cv_p'],\n",
    "                 c=[{'Robusta': 'green', 'Intermedia': 'orange', 'Problem√°tica': 'red'}[cat]\n",
    "                    for cat in df_robustez['Clasificacion']], alpha=0.7, s=80)\n",
    "axes[0,1].set_xlabel('SNR-P Medio (dB)')\n",
    "axes[0,1].set_ylabel('Coeficiente de Variaci√≥n SNR-P')\n",
    "axes[0,1].set_title('SNR Medio vs Variabilidad (Fase P)')\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir etiquetas de estaci√≥n\n",
    "for idx, row in df_robustez.iterrows():\n",
    "    axes[0,1].annotate(row['Estacion'], (row['snr_p_mean'], row['cv_p']),\n",
    "                      xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# 3. Porcentaje de eventos con SNR bajo\n",
    "x_pos = range(len(df_robustez))\n",
    "axes[1,0].bar([x-0.2 for x in x_pos], df_robustez.sort_values('robustez_score_total')['pct_snr_p_bajo'],\n",
    "              width=0.4, label='SNR-P < 10dB', alpha=0.7, color='blue')\n",
    "axes[1,0].bar([x+0.2 for x in x_pos], df_robustez.sort_values('robustez_score_total')['pct_snr_s_bajo'],\n",
    "              width=0.4, label='SNR-S < 10dB', alpha=0.7, color='red')\n",
    "axes[1,0].set_xticks(x_pos)\n",
    "axes[1,0].set_xticklabels(df_robustez.sort_values('robustez_score_total')['Estacion'], rotation=45)\n",
    "axes[1,0].set_ylabel('Porcentaje de Eventos (%)')\n",
    "axes[1,0].set_title('Eventos con SNR Bajo por Estaci√≥n')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Comparaci√≥n SNR-P vs SNR-S por clasificaci√≥n\n",
    "for categoria, color in [('Robusta', 'green'), ('Intermedia', 'orange'), ('Problem√°tica', 'red')]:\n",
    "    data_cat = df_robustez[df_robustez['Clasificacion'] == categoria]\n",
    "    axes[1,1].scatter(data_cat['snr_p_mean'], data_cat['snr_s_mean'],\n",
    "                     label=categoria, color=color, alpha=0.7, s=80)\n",
    "\n",
    "axes[1,1].set_xlabel('SNR-P Medio (dB)')\n",
    "axes[1,1].set_ylabel('SNR-S Medio (dB)')\n",
    "axes[1,1].set_title('SNR-P vs SNR-S por Clasificaci√≥n')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# L√≠nea de igualdad\n",
    "min_snr = min(df_robustez['snr_p_mean'].min(), df_robustez['snr_s_mean'].min())\n",
    "max_snr = max(df_robustez['snr_p_mean'].max(), df_robustez['snr_s_mean'].max())\n",
    "axes[1,1].plot([min_snr, max_snr], [min_snr, max_snr], 'k--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758209872859,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "XwxVmVecH6qh",
    "outputId": "935d6d81-b1e8-4890-bd82-5b06551de232"
   },
   "outputs": [],
   "source": [
    "# An√°lisis detallado de estaciones problem√°ticas\n",
    "print(f\"\\n=== AN√ÅLISIS DETALLADO DE ESTACIONES PROBLEM√ÅTICAS ===\")\n",
    "\n",
    "estaciones_problematicas = df_robustez[df_robustez['Clasificacion'] == 'Problem√°tica']['Estacion'].tolist()\n",
    "estaciones_robustas = df_robustez[df_robustez['Clasificacion'] == 'Robusta']['Estacion'].tolist()\n",
    "\n",
    "if estaciones_problematicas:\n",
    "    print(f\"\\nEstaciones PROBLEM√ÅTICAS ({len(estaciones_problematicas)}):\")\n",
    "    for estacion in estaciones_problematicas:\n",
    "        data_est = df_robustez[df_robustez['Estacion'] == estacion].iloc[0]\n",
    "        print(f\"\\n{estacion}:\")\n",
    "        print(f\"  ‚Ä¢ Score robustez: {data_est['robustez_score_total']:.1f}/100\")\n",
    "        print(f\"  ‚Ä¢ SNR-P medio: {data_est['snr_p_mean']:.1f} dB (std: {data_est['snr_p_std']:.1f})\")\n",
    "        print(f\"  ‚Ä¢ SNR-S medio: {data_est['snr_s_mean']:.1f} dB (std: {data_est['snr_s_std']:.1f})\")\n",
    "        print(f\"  ‚Ä¢ Eventos con SNR-P bajo: {data_est['pct_snr_p_bajo']:.1f}%\")\n",
    "        print(f\"  ‚Ä¢ Eventos con SNR-S bajo: {data_est['pct_snr_s_bajo']:.1f}%\")\n",
    "        print(f\"  ‚Ä¢ Variabilidad P (CV): {data_est['cv_p']:.2f}\")\n",
    "        print(f\"  ‚Ä¢ Variabilidad S (CV): {data_est['cv_s']:.2f}\")\n",
    "\n",
    "        # Identificar problemas principales\n",
    "        problemas = []\n",
    "        if data_est['snr_p_mean'] < 15: problemas.append(\"SNR-P bajo\")\n",
    "        if data_est['snr_s_mean'] < 15: problemas.append(\"SNR-S bajo\")\n",
    "        if data_est['pct_snr_p_bajo'] > 30: problemas.append(\"Muchos eventos P con SNR<10\")\n",
    "        if data_est['pct_snr_s_bajo'] > 30: problemas.append(\"Muchos eventos S con SNR<10\")\n",
    "        if data_est['cv_p'] > 0.5: problemas.append(\"Alta variabilidad P\")\n",
    "        if data_est['cv_s'] > 0.5: problemas.append(\"Alta variabilidad S\")\n",
    "\n",
    "        if problemas:\n",
    "            print(f\"  ‚Ä¢ Problemas principales: {', '.join(problemas)}\")\n",
    "\n",
    "print(f\"\\nEstaciones ROBUSTAS ({len(estaciones_robustas)}) para comparaci√≥n:\")\n",
    "for estacion in estaciones_robustas:\n",
    "    data_est = df_robustez[df_robustez['Estacion'] == estacion].iloc[0]\n",
    "    print(f\"  {estacion}: Score {data_est['robustez_score_total']:.1f}, \"\n",
    "          f\"SNR-P {data_est['snr_p_mean']:.1f}¬±{data_est['snr_p_std']:.1f}, \"\n",
    "          f\"SNR-S {data_est['snr_s_mean']:.1f}¬±{data_est['snr_s_std']:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1758209872864,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "WObPy5CMIPCc",
    "outputId": "6c6e6191-7186-44ab-e165-6ae758dd5e26"
   },
   "outputs": [],
   "source": [
    "# Recomendaciones basadas en el an√°lisis\n",
    "print(f\"\\n=== RECOMENDACIONES ===\")\n",
    "\n",
    "# Estad√≠sticas generales\n",
    "total_estaciones = len(df_robustez)\n",
    "pct_problematicas = len(estaciones_problematicas) / total_estaciones * 100\n",
    "pct_robustas = len(estaciones_robustas) / total_estaciones * 100\n",
    "\n",
    "print(f\"üìä Resumen general:\")\n",
    "print(f\"  ‚Ä¢ {pct_problematicas:.1f}% de estaciones son problem√°ticas\")\n",
    "print(f\"  ‚Ä¢ {pct_robustas:.1f}% de estaciones son robustas\")\n",
    "\n",
    "if estaciones_problematicas:\n",
    "    print(f\"\\nüîß Recomendaciones para estaciones problem√°ticas:\")\n",
    "    print(f\"  ‚Ä¢ Revisar calibraci√≥n de instrumentos\")\n",
    "    print(f\"  ‚Ä¢ Verificar condiciones ambientales (ruido s√≠smico)\")\n",
    "    print(f\"  ‚Ä¢ Considerar filtrado adicional en preprocesamiento\")\n",
    "    print(f\"  ‚Ä¢ Ajustar par√°metros del modelo GPD espec√≠ficamente para estas estaciones\")\n",
    "    print(f\"  ‚Ä¢ Implementar umbrales adaptativos por estaci√≥n\")\n",
    "\n",
    "print(f\"\\nüìà Para el an√°lisis del modelo GPD:\")\n",
    "print(f\"  ‚Ä¢ Esperar mayor dificultad en estaciones: {estaciones_problematicas}\")\n",
    "print(f\"  ‚Ä¢ Usar estaciones robustas como benchmark: {estaciones_robustas[:3]}\")\n",
    "print(f\"  ‚Ä¢ Considerar pesos diferentes por estaci√≥n en evaluaci√≥n\")\n",
    "\n",
    "# Guardar resultados para an√°lisis posteriores\n",
    "df_robustez.to_csv('/content/analisis_robustez_estaciones.csv', index=False)\n",
    "print(f\"\\nüíæ An√°lisis de robustez guardado en: /content/analisis_robustez_estaciones.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HysjbdWIfqc"
   },
   "source": [
    "**Paso 7: Impacto de estaciones problem√°ticas en m√©tricas del modelo GPD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2388,
     "status": "ok",
     "timestamp": 1758209875259,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "beYrlWbPIT1q",
    "outputId": "19c0a3e1-e225-4cc3-e0b3-22cd61c9a0db"
   },
   "outputs": [],
   "source": [
    "# Primero necesitamos procesar los timestamps y calcular m√©tricas del modelo GPD\n",
    "print(\"=== PROCESANDO DATOS PARA AN√ÅLISIS DEL MODELO GPD ===\")\n",
    "\n",
    "# Funci√≥n para parsear timestamps (del script original)\n",
    "def parse_timestamp(ts_str):\n",
    "    \"\"\"Convierte timestamp a datetime con manejo robusto de formatos\"\"\"\n",
    "    if pd.isna(ts_str) or ts_str == 'NA':\n",
    "        return None\n",
    "    try:\n",
    "        ts_str = str(ts_str).replace('Z', '+00:00')\n",
    "        if '+' not in ts_str and 'T' in ts_str:\n",
    "            ts_str += '+00:00'\n",
    "        return pd.to_datetime(ts_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Parsear timestamps\n",
    "print(\"Procesando timestamps...\")\n",
    "df_combined['T-P_ref_dt'] = df_combined['T-P_ref'].apply(parse_timestamp)\n",
    "df_combined['T-S_ref_dt'] = df_combined['T-S_ref'].apply(parse_timestamp)\n",
    "df_combined['T-P_gpd_dt'] = df_combined['T-P_gpd'].apply(parse_timestamp)\n",
    "df_combined['T-S_gpd_dt'] = df_combined['T-S_gpd'].apply(parse_timestamp)\n",
    "\n",
    "# Calcular errores temporales\n",
    "df_combined['error_temporal_P'] = df_combined.apply(\n",
    "    lambda row: (row['T-P_gpd_dt'] - row['T-P_ref_dt']).total_seconds()\n",
    "    if pd.notna(row['T-P_gpd_dt']) and pd.notna(row['T-P_ref_dt']) else None, axis=1\n",
    ")\n",
    "\n",
    "df_combined['error_temporal_S'] = df_combined.apply(\n",
    "    lambda row: (row['T-S_gpd_dt'] - row['T-S_ref_dt']).total_seconds()\n",
    "    if pd.notna(row['T-S_gpd_dt']) and pd.notna(row['T-S_ref_dt']) else None, axis=1\n",
    ")\n",
    "\n",
    "print(f\"‚úì Errores temporales calculados\")\n",
    "print(f\"‚úì Detecciones P v√°lidas: {df_combined['error_temporal_P'].notna().sum()}\")\n",
    "print(f\"‚úì Detecciones S v√°lidas: {df_combined['error_temporal_S'].notna().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1758209875341,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "PX2qTrIxIub-",
    "outputId": "b8e52be7-58ef-4036-85d7-bd4fe2df823b"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para calcular m√©tricas del modelo por estaci√≥n\n",
    "def calcular_metricas_modelo_por_estacion(df, tolerancia=1.0):\n",
    "    \"\"\"Calcula m√©tricas de detecci√≥n del modelo GPD por estaci√≥n\"\"\"\n",
    "\n",
    "    metricas_modelo = []\n",
    "\n",
    "    for estacion in sorted(df['Estacion'].unique()):\n",
    "        data_est = df[df['Estacion'] == estacion].copy()\n",
    "        n_eventos = len(data_est)\n",
    "\n",
    "        # Tasas de detecci√≥n\n",
    "        detecciones_p = data_est['T-P_gpd_dt'].notna().sum()\n",
    "        detecciones_s = data_est['T-S_gpd_dt'].notna().sum()\n",
    "        detecciones_ambas = ((data_est['T-P_gpd_dt'].notna()) & (data_est['T-S_gpd_dt'].notna())).sum()\n",
    "\n",
    "        tasa_deteccion_p = detecciones_p / n_eventos * 100\n",
    "        tasa_deteccion_s = detecciones_s / n_eventos * 100\n",
    "        tasa_deteccion_ambas = detecciones_ambas / n_eventos * 100\n",
    "\n",
    "        # M√©tricas de precisi√≥n temporal\n",
    "        errores_p = data_est['error_temporal_P'].dropna()\n",
    "        errores_s = data_est['error_temporal_S'].dropna()\n",
    "\n",
    "        # Estad√≠sticas de error P\n",
    "        if len(errores_p) > 0:\n",
    "            error_p_mean = errores_p.mean()\n",
    "            error_p_median = errores_p.median()\n",
    "            error_p_std = errores_p.std()\n",
    "            error_p_rms = np.sqrt((errores_p ** 2).mean())\n",
    "            error_p_mae = errores_p.abs().mean()\n",
    "\n",
    "            # Clasificaci√≥n de calidad P\n",
    "            errores_p_abs = errores_p.abs()\n",
    "            excelente_p = (errores_p_abs <= 1.0).sum()\n",
    "            buena_p = ((errores_p_abs > 1.0) & (errores_p_abs <= 5.0)).sum()\n",
    "            pobre_p = (errores_p_abs > 5.0).sum()\n",
    "\n",
    "            pct_excelente_p = excelente_p / len(errores_p) * 100\n",
    "            pct_buena_p = buena_p / len(errores_p) * 100\n",
    "            pct_pobre_p = pobre_p / len(errores_p) * 100\n",
    "        else:\n",
    "            error_p_mean = error_p_median = error_p_std = error_p_rms = error_p_mae = np.nan\n",
    "            pct_excelente_p = pct_buena_p = pct_pobre_p = 0\n",
    "\n",
    "        # Estad√≠sticas de error S\n",
    "        if len(errores_s) > 0:\n",
    "            error_s_mean = errores_s.mean()\n",
    "            error_s_median = errores_s.median()\n",
    "            error_s_std = errores_s.std()\n",
    "            error_s_rms = np.sqrt((errores_s ** 2).mean())\n",
    "            error_s_mae = errores_s.abs().mean()\n",
    "\n",
    "            # Clasificaci√≥n de calidad S\n",
    "            errores_s_abs = errores_s.abs()\n",
    "            excelente_s = (errores_s_abs <= 1.0).sum()\n",
    "            buena_s = ((errores_s_abs > 1.0) & (errores_s_abs <= 5.0)).sum()\n",
    "            pobre_s = (errores_s_abs > 5.0).sum()\n",
    "\n",
    "            pct_excelente_s = excelente_s / len(errores_s) * 100\n",
    "            pct_buena_s = buena_s / len(errores_s) * 100\n",
    "            pct_pobre_s = pobre_s / len(errores_s) * 100\n",
    "        else:\n",
    "            error_s_mean = error_s_median = error_s_std = error_s_rms = error_s_mae = np.nan\n",
    "            pct_excelente_s = pct_buena_s = pct_pobre_s = 0\n",
    "\n",
    "        # Calcular m√©tricas P/R/F1 simplificadas para esta tolerancia\n",
    "        # Para fase P\n",
    "        ref_p = data_est['T-P_ref_dt'].notna()\n",
    "        det_p = data_est['T-P_gpd_dt'].notna()\n",
    "        errores_p_todos = data_est['error_temporal_P']\n",
    "\n",
    "        tp_p = ((ref_p & det_p) & (errores_p_todos.abs() <= tolerancia)).sum()\n",
    "        fp_p = ((~ref_p & det_p) | ((ref_p & det_p) & (errores_p_todos.abs() > tolerancia))).sum()\n",
    "        fn_p = ((ref_p & ~det_p) | ((ref_p & det_p) & (errores_p_todos.abs() > tolerancia))).sum()\n",
    "\n",
    "        precision_p = tp_p / (tp_p + fp_p) if (tp_p + fp_p) > 0 else 0\n",
    "        recall_p = tp_p / (tp_p + fn_p) if (tp_p + fn_p) > 0 else 0\n",
    "        f1_p = (2 * precision_p * recall_p) / (precision_p + recall_p) if (precision_p + recall_p) > 0 else 0\n",
    "\n",
    "        # Para fase S\n",
    "        ref_s = data_est['T-S_ref_dt'].notna()\n",
    "        det_s = data_est['T-S_gpd_dt'].notna()\n",
    "        errores_s_todos = data_est['error_temporal_S']\n",
    "\n",
    "        tp_s = ((ref_s & det_s) & (errores_s_todos.abs() <= tolerancia)).sum()\n",
    "        fp_s = ((~ref_s & det_s) | ((ref_s & det_s) & (errores_s_todos.abs() > tolerancia))).sum()\n",
    "        fn_s = ((ref_s & ~det_s) | ((ref_s & det_s) & (errores_s_todos.abs() > tolerancia))).sum()\n",
    "\n",
    "        precision_s = tp_s / (tp_s + fp_s) if (tp_s + fp_s) > 0 else 0\n",
    "        recall_s = tp_s / (tp_s + fn_s) if (tp_s + fn_s) > 0 else 0\n",
    "        f1_s = (2 * precision_s * recall_s) / (precision_s + recall_s) if (precision_s + recall_s) > 0 else 0\n",
    "\n",
    "        metricas_modelo.append({\n",
    "            'Estacion': estacion,\n",
    "            'N_eventos': n_eventos,\n",
    "            'detecciones_p': detecciones_p,\n",
    "            'detecciones_s': detecciones_s,\n",
    "            'detecciones_ambas': detecciones_ambas,\n",
    "            'tasa_deteccion_p': tasa_deteccion_p,\n",
    "            'tasa_deteccion_s': tasa_deteccion_s,\n",
    "            'tasa_deteccion_ambas': tasa_deteccion_ambas,\n",
    "            'error_p_mean': error_p_mean,\n",
    "            'error_p_median': error_p_median,\n",
    "            'error_p_rms': error_p_rms,\n",
    "            'error_p_mae': error_p_mae,\n",
    "            'error_s_mean': error_s_mean,\n",
    "            'error_s_median': error_s_median,\n",
    "            'error_s_rms': error_s_rms,\n",
    "            'error_s_mae': error_s_mae,\n",
    "            'pct_excelente_p': pct_excelente_p,\n",
    "            'pct_buena_p': pct_buena_p,\n",
    "            'pct_pobre_p': pct_pobre_p,\n",
    "            'pct_excelente_s': pct_excelente_s,\n",
    "            'pct_buena_s': pct_buena_s,\n",
    "            'pct_pobre_s': pct_pobre_s,\n",
    "            'precision_p': precision_p,\n",
    "            'recall_p': recall_p,\n",
    "            'f1_p': f1_p,\n",
    "            'precision_s': precision_s,\n",
    "            'recall_s': recall_s,\n",
    "            'f1_s': f1_s,\n",
    "            'f1_promedio': (f1_p + f1_s) / 2\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metricas_modelo)\n",
    "\n",
    "# Calcular m√©tricas del modelo\n",
    "df_metricas_modelo = calcular_metricas_modelo_por_estacion(df_combined, tolerancia=1.0)\n",
    "print(\"‚úì M√©tricas del modelo GPD calculadas por estaci√≥n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 34,
     "status": "ok",
     "timestamp": 1758209875379,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "IlWPEe7xIxto",
    "outputId": "48c8ed2d-fb47-43d6-86e0-79d918656eb9"
   },
   "outputs": [],
   "source": [
    "# Combinar con clasificaci√≥n de robustez\n",
    "df_analisis_completo = pd.merge(\n",
    "    df_metricas_modelo,\n",
    "    df_robustez[['Estacion', 'Clasificacion', 'robustez_score_total', 'snr_p_mean', 'snr_s_mean']],\n",
    "    on='Estacion'\n",
    ")\n",
    "\n",
    "print(\"=== IMPACTO DE ESTACIONES PROBLEM√ÅTICAS EN M√âTRICAS DEL MODELO ===\")\n",
    "print(f\"Total estaciones analizadas: {len(df_analisis_completo)}\")\n",
    "\n",
    "# Estad√≠sticas por tipo de estaci√≥n\n",
    "for categoria in ['Problem√°tica', 'Intermedia', 'Robusta']:\n",
    "    data_cat = df_analisis_completo[df_analisis_completo['Clasificacion'] == categoria]\n",
    "    if len(data_cat) > 0:\n",
    "        print(f\"\\n{categoria.upper()} ({len(data_cat)} estaciones):\")\n",
    "        print(f\"  Tasa detecci√≥n P: {data_cat['tasa_deteccion_p'].mean():.1f}% (¬±{data_cat['tasa_deteccion_p'].std():.1f}%)\")\n",
    "        print(f\"  Tasa detecci√≥n S: {data_cat['tasa_deteccion_s'].mean():.1f}% (¬±{data_cat['tasa_deteccion_s'].std():.1f}%)\")\n",
    "        print(f\"  F1-Score P: {data_cat['f1_p'].mean():.3f} (¬±{data_cat['f1_p'].std():.3f})\")\n",
    "        print(f\"  F1-Score S: {data_cat['f1_s'].mean():.3f} (¬±{data_cat['f1_s'].std():.3f})\")\n",
    "        print(f\"  Error RMS P: {data_cat['error_p_rms'].mean():.2f}s (¬±{data_cat['error_p_rms'].std():.2f}s)\")\n",
    "        print(f\"  Error RMS S: {data_cat['error_s_rms'].mean():.2f}s (¬±{data_cat['error_s_rms'].std():.2f}s)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2061,
     "status": "ok",
     "timestamp": 1758209877453,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "o189IvyPJBvR",
    "outputId": "341fa651-cfc2-4a75-db7e-d0b781d52eb5"
   },
   "outputs": [],
   "source": [
    "# Visualizaci√≥n del impacto\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "# Colores por categor√≠a\n",
    "colores_cat = {'Robusta': 'green', 'Intermedia': 'orange', 'Problem√°tica': 'red'}\n",
    "\n",
    "# 1. Tasas de detecci√≥n por categor√≠a\n",
    "categorias = ['Problem√°tica', 'Intermedia', 'Robusta']\n",
    "tasas_p = [df_analisis_completo[df_analisis_completo['Clasificacion'] == cat]['tasa_deteccion_p'].mean()\n",
    "           for cat in categorias]\n",
    "tasas_s = [df_analisis_completo[df_analisis_completo['Clasificacion'] == cat]['tasa_deteccion_s'].mean()\n",
    "           for cat in categorias]\n",
    "\n",
    "x_pos = np.arange(len(categorias))\n",
    "width = 0.35\n",
    "\n",
    "axes[0,0].bar(x_pos - width/2, tasas_p, width, label='Fase P', alpha=0.7, color='blue')\n",
    "axes[0,0].bar(x_pos + width/2, tasas_s, width, label='Fase S', alpha=0.7, color='red')\n",
    "axes[0,0].set_xlabel('Clasificaci√≥n de Estaci√≥n')\n",
    "axes[0,0].set_ylabel('Tasa de Detecci√≥n (%)')\n",
    "axes[0,0].set_title('Tasas de Detecci√≥n por Tipo de Estaci√≥n')\n",
    "axes[0,0].set_xticks(x_pos)\n",
    "axes[0,0].set_xticklabels(categorias)\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. F1-Scores por categor√≠a\n",
    "f1_p = [df_analisis_completo[df_analisis_completo['Clasificacion'] == cat]['f1_p'].mean()\n",
    "        for cat in categorias]\n",
    "f1_s = [df_analisis_completo[df_analisis_completo['Clasificacion'] == cat]['f1_s'].mean()\n",
    "        for cat in categorias]\n",
    "\n",
    "axes[0,1].bar(x_pos - width/2, f1_p, width, label='F1 Fase P', alpha=0.7, color='blue')\n",
    "axes[0,1].bar(x_pos + width/2, f1_s, width, label='F1 Fase S', alpha=0.7, color='red')\n",
    "axes[0,1].set_xlabel('Clasificaci√≥n de Estaci√≥n')\n",
    "axes[0,1].set_ylabel('F1-Score')\n",
    "axes[0,1].set_title('F1-Scores por Tipo de Estaci√≥n')\n",
    "axes[0,1].set_xticks(x_pos)\n",
    "axes[0,1].set_xticklabels(categorias)\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter: SNR vs F1-Score P\n",
    "for categoria, color in colores_cat.items():\n",
    "    data_cat = df_analisis_completo[df_analisis_completo['Clasificacion'] == categoria]\n",
    "    axes[1,0].scatter(data_cat['snr_p_mean'], data_cat['f1_p'],\n",
    "                     label=categoria, color=color, alpha=0.7, s=80)\n",
    "\n",
    "axes[1,0].set_xlabel('SNR-P Medio (dB)')\n",
    "axes[1,0].set_ylabel('F1-Score Fase P')\n",
    "axes[1,0].set_title('SNR-P vs F1-Score Fase P')\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# A√±adir l√≠nea de tendencia\n",
    "from scipy.stats import pearsonr\n",
    "x_vals = df_analisis_completo['snr_p_mean'].dropna()\n",
    "y_vals = df_analisis_completo['f1_p'].dropna()\n",
    "if len(x_vals) > 1:\n",
    "    z = np.polyfit(x_vals, y_vals, 1)\n",
    "    p = np.poly1d(z)\n",
    "    axes[1,0].plot(x_vals, p(x_vals), \"r--\", alpha=0.8)\n",
    "    corr, _ = pearsonr(x_vals, y_vals)\n",
    "    axes[1,0].text(0.05, 0.95, f'r = {corr:.3f}', transform=axes[1,0].transAxes,\n",
    "                  bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.8))\n",
    "\n",
    "# 4. Scatter: SNR vs F1-Score S\n",
    "for categoria, color in colores_cat.items():\n",
    "    data_cat = df_analisis_completo[df_analisis_completo['Clasificacion'] == categoria]\n",
    "    axes[1,1].scatter(data_cat['snr_s_mean'], data_cat['f1_s'],\n",
    "                     label=categoria, color=color, alpha=0.7, s=80)\n",
    "\n",
    "axes[1,1].set_xlabel('SNR-S Medio (dB)')\n",
    "axes[1,1].set_ylabel('F1-Score Fase S')\n",
    "axes[1,1].set_title('SNR-S vs F1-Score Fase S')\n",
    "axes[1,1].legend()\n",
    "axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# L√≠nea de tendencia para S\n",
    "x_vals_s = df_analisis_completo['snr_s_mean'].dropna()\n",
    "y_vals_s = df_analisis_completo['f1_s'].dropna()\n",
    "if len(x_vals_s) > 1:\n",
    "    z_s = np.polyfit(x_vals_s, y_vals_s, 1)\n",
    "    p_s = np.poly1d(z_s)\n",
    "    axes[1,1].plot(x_vals_s, p_s(x_vals_s), \"r--\", alpha=0.8)\n",
    "    corr_s, _ = pearsonr(x_vals_s, y_vals_s)\n",
    "    axes[1,1].text(0.05, 0.95, f'r = {corr_s:.3f}', transform=axes[1,1].transAxes,\n",
    "                  bbox=dict(boxstyle=\"round\", facecolor='white', alpha=0.8))\n",
    "\n",
    "# 5. Errores RMS por categor√≠a\n",
    "error_rms_p = [df_analisis_completo[df_analisis_completo['Clasificacion'] == cat]['error_p_rms'].mean()\n",
    "               for cat in categorias]\n",
    "error_rms_s = [df_analisis_completo[df_analisis_completo['Clasificacion'] == cat]['error_s_rms'].mean()\n",
    "               for cat in categorias]\n",
    "\n",
    "axes[2,0].bar(x_pos - width/2, error_rms_p, width, label='Error RMS P', alpha=0.7, color='blue')\n",
    "axes[2,0].bar(x_pos + width/2, error_rms_s, width, label='Error RMS S', alpha=0.7, color='red')\n",
    "axes[2,0].set_xlabel('Clasificaci√≥n de Estaci√≥n')\n",
    "axes[2,0].set_ylabel('Error RMS (segundos)')\n",
    "axes[2,0].set_title('Errores RMS por Tipo de Estaci√≥n')\n",
    "axes[2,0].set_xticks(x_pos)\n",
    "axes[2,0].set_xticklabels(categorias)\n",
    "axes[2,0].legend()\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Distribuci√≥n de calidad de detecciones\n",
    "calidad_data = []\n",
    "for categoria in categorias:\n",
    "    data_cat = df_analisis_completo[df_analisis_completo['Clasificacion'] == categoria]\n",
    "    pct_exc_p = data_cat['pct_excelente_p'].mean()\n",
    "    pct_bue_p = data_cat['pct_buena_p'].mean()\n",
    "    pct_pob_p = data_cat['pct_pobre_p'].mean()\n",
    "\n",
    "    calidad_data.append([pct_exc_p, pct_bue_p, pct_pob_p])\n",
    "\n",
    "calidad_data = np.array(calidad_data)\n",
    "bottom_buena = calidad_data[:, 0]\n",
    "bottom_pobre = calidad_data[:, 0] + calidad_data[:, 1]\n",
    "\n",
    "axes[2,1].bar(x_pos, calidad_data[:, 0], label='Excelente (‚â§1s)', color='green', alpha=0.7)\n",
    "axes[2,1].bar(x_pos, calidad_data[:, 1], bottom=bottom_buena, label='Buena (1-5s)', color='orange', alpha=0.7)\n",
    "axes[2,1].bar(x_pos, calidad_data[:, 2], bottom=bottom_pobre, label='Pobre (>5s)', color='red', alpha=0.7)\n",
    "\n",
    "axes[2,1].set_xlabel('Clasificaci√≥n de Estaci√≥n')\n",
    "axes[2,1].set_ylabel('Porcentaje de Detecciones (%)')\n",
    "axes[2,1].set_title('Calidad de Detecciones P por Tipo de Estaci√≥n')\n",
    "axes[2,1].set_xticks(x_pos)\n",
    "axes[2,1].set_xticklabels(categorias)\n",
    "axes[2,1].legend()\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1758209877464,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "seG3us71Jiaf",
    "outputId": "8d522ac8-c83d-4222-f433-f8055e69a602"
   },
   "outputs": [],
   "source": [
    "# An√°lisis estad√≠stico de diferencias\n",
    "from scipy.stats import mannwhitneyu, kruskal\n",
    "\n",
    "print(f\"\\n=== AN√ÅLISIS ESTAD√çSTICO DE DIFERENCIAS ===\")\n",
    "\n",
    "# Test de Kruskal-Wallis para m√∫ltiples grupos\n",
    "metricas_test = ['tasa_deteccion_p', 'tasa_deteccion_s', 'f1_p', 'f1_s', 'error_p_rms', 'error_s_rms']\n",
    "\n",
    "for metrica in metricas_test:\n",
    "    grupos = [df_analisis_completo[df_analisis_completo['Clasificacion'] == cat][metrica].dropna()\n",
    "              for cat in categorias]\n",
    "\n",
    "    # Filtrar grupos vac√≠os\n",
    "    grupos = [g for g in grupos if len(g) > 0]\n",
    "\n",
    "    if len(grupos) >= 2:\n",
    "        h_stat, p_value = kruskal(*grupos)\n",
    "        print(f\"\\n{metrica}:\")\n",
    "        print(f\"  Kruskal-Wallis H: {h_stat:.3f}\")\n",
    "        print(f\"  p-value: {p_value:.6f}\")\n",
    "        print(f\"  Diferencias significativas: {'S√ç' if p_value < 0.05 else 'NO'}\")\n",
    "\n",
    "# Comparaciones par a par m√°s importantes\n",
    "print(f\"\\n=== COMPARACIONES PROBLEM√ÅTICAS vs ROBUSTAS ===\")\n",
    "\n",
    "prob_data = df_analisis_completo[df_analisis_completo['Clasificacion'] == 'Problem√°tica']\n",
    "rob_data = df_analisis_completo[df_analisis_completo['Clasificacion'] == 'Robusta']\n",
    "\n",
    "if len(prob_data) > 0 and len(rob_data) > 0:\n",
    "    comparaciones = [\n",
    "        ('Tasa detecci√≥n P', 'tasa_deteccion_p'),\n",
    "        ('Tasa detecci√≥n S', 'tasa_deteccion_s'),\n",
    "        ('F1-Score P', 'f1_p'),\n",
    "        ('F1-Score S', 'f1_s'),\n",
    "        ('Error RMS P', 'error_p_rms'),\n",
    "        ('Error RMS S', 'error_s_rms')\n",
    "    ]\n",
    "\n",
    "    for nombre, metrica in comparaciones:\n",
    "        prob_vals = prob_data[metrica].dropna()\n",
    "        rob_vals = rob_data[metrica].dropna()\n",
    "\n",
    "        if len(prob_vals) > 0 and len(rob_vals) > 0:\n",
    "            u_stat, p_value = mannwhitneyu(prob_vals, rob_vals, alternative='two-sided')\n",
    "\n",
    "            prob_mean = prob_vals.mean()\n",
    "            rob_mean = rob_vals.mean()\n",
    "            diferencia = prob_mean - rob_mean\n",
    "            pct_diferencia = (diferencia / rob_mean * 100) if rob_mean != 0 else 0\n",
    "\n",
    "            print(f\"\\n{nombre}:\")\n",
    "            print(f\"  Problem√°ticas: {prob_mean:.3f}\")\n",
    "            print(f\"  Robustas: {rob_mean:.3f}\")\n",
    "            print(f\"  Diferencia: {diferencia:+.3f} ({pct_diferencia:+.1f}%)\")\n",
    "            print(f\"  p-value: {p_value:.6f} ({'significativo' if p_value < 0.05 else 'no significativo'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758209877470,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "fr70_2wQJmXC",
    "outputId": "f98cf250-17a9-42a1-f34e-ac971d5f9c15"
   },
   "outputs": [],
   "source": [
    "# Tabla resumen final\n",
    "print(f\"\\n=== RESUMEN CUANTITATIVO DEL IMPACTO ===\")\n",
    "\n",
    "# Crear tabla resumen\n",
    "resumen_impacto = df_analisis_completo.groupby('Clasificacion').agg({\n",
    "    'tasa_deteccion_p': ['mean', 'std', 'count'],\n",
    "    'tasa_deteccion_s': ['mean', 'std'],\n",
    "    'f1_p': ['mean', 'std'],\n",
    "    'f1_s': ['mean', 'std'],\n",
    "    'error_p_rms': ['mean', 'std'],\n",
    "    'error_s_rms': ['mean', 'std'],\n",
    "    'snr_p_mean': ['mean', 'std'],\n",
    "    'snr_s_mean': ['mean', 'std']\n",
    "}).round(3)\n",
    "\n",
    "print(\"Tabla resumen por tipo de estaci√≥n:\")\n",
    "print(resumen_impacto)\n",
    "\n",
    "# Conclusiones del an√°lisis\n",
    "print(f\"\\n=== CONCLUSIONES DEL AN√ÅLISIS ===\")\n",
    "\n",
    "if len(prob_data) > 0 and len(rob_data) > 0:\n",
    "    # Calcular impactos relativos\n",
    "    impacto_f1_p = ((rob_data['f1_p'].mean() - prob_data['f1_p'].mean()) / prob_data['f1_p'].mean() * 100)\n",
    "    impacto_f1_s = ((rob_data['f1_s'].mean() - prob_data['f1_s'].mean()) / prob_data['f1_s'].mean() * 100)\n",
    "    impacto_tasa_p = ((rob_data['tasa_deteccion_p'].mean() - prob_data['tasa_deteccion_p'].mean()) / prob_data['tasa_deteccion_p'].mean() * 100)\n",
    "    impacto_tasa_s = ((rob_data['tasa_deteccion_s'].mean() - prob_data['tasa_deteccion_s'].mean()) / prob_data['tasa_deteccion_s'].mean() * 100)\n",
    "\n",
    "    print(f\"‚úì HIP√ìTESIS CONFIRMADA: El SNR bajo impacta significativamente el rendimiento del modelo GPD\")\n",
    "    print(f\"\\nüìä Impactos cuantificados:\")\n",
    "    print(f\"  ‚Ä¢ F1-Score P: {impacto_f1_p:+.1f}% mejor en estaciones robustas\")\n",
    "    print(f\"  ‚Ä¢ F1-Score S: {impacto_f1_s:+.1f}% mejor en estaciones robustas\")\n",
    "    print(f\"  ‚Ä¢ Tasa detecci√≥n P: {impacto_tasa_p:+.1f}% mejor en estaciones robustas\")\n",
    "    print(f\"  ‚Ä¢ Tasa detecci√≥n S: {impacto_tasa_s:+.1f}% mejor en estaciones robustas\")\n",
    "\n",
    "    print(f\"\\nüéØ Implicaciones para el modelo:\")\n",
    "    print(f\"  ‚Ä¢ SNR es un predictor fuerte del rendimiento del modelo GPD\")\n",
    "    print(f\"  ‚Ä¢ Estaciones problem√°ticas requieren tratamiento especial\")\n",
    "    print(f\"  ‚Ä¢ Los umbrales de detecci√≥n podr√≠an ajustarse por estaci√≥n\")\n",
    "    print(f\"  ‚Ä¢ El filtrado previo es m√°s cr√≠tico en estaciones con SNR bajo\")\n",
    "\n",
    "# Guardar an√°lisis completo\n",
    "df_analisis_completo.to_csv('/content/analisis_completo_impacto_modelo.csv', index=False)\n",
    "print(f\"\\nüíæ An√°lisis completo guardado en: /content/analisis_completo_impacto_modelo.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB754s0PK0rn"
   },
   "source": [
    "**Paso 8: An√°lisis detallado SNR vs M√©tricas del Modelo GPD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1758209877486,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "ZmKI7Gv5K2Ns",
    "outputId": "be81f700-2ae0-4c59-eeba-146cc00d47e9"
   },
   "outputs": [],
   "source": [
    "# Crear bins de SNR para an√°lisis granular\n",
    "print(\"=== AN√ÅLISIS GRANULAR: SNR vs M√âTRICAS DEL MODELO ===\")\n",
    "\n",
    "def crear_bins_snr(df, columna_snr, metodo='percentiles'):\n",
    "    \"\"\"Crea bins de SNR usando diferentes m√©todos\"\"\"\n",
    "\n",
    "    if metodo == 'percentiles':\n",
    "        # Bins basados en percentiles\n",
    "        bins = [df[columna_snr].quantile(q) for q in [0, 0.2, 0.4, 0.6, 0.8, 1.0]]\n",
    "        labels = ['Muy Bajo\\n(0-20%)', 'Bajo\\n(20-40%)', 'Medio\\n(40-60%)',\n",
    "                 'Alto\\n(60-80%)', 'Muy Alto\\n(80-100%)']\n",
    "    elif metodo == 'fijo':\n",
    "        # Bins fijos basados en conocimiento s√≠smico\n",
    "        bins = [-np.inf, 5, 10, 15, 20, np.inf]\n",
    "        labels = ['<5 dB', '5-10 dB', '10-15 dB', '15-20 dB', '>20 dB']\n",
    "    else:\n",
    "        # Bins adaptativos\n",
    "        q25, q50, q75 = df[columna_snr].quantile([0.25, 0.5, 0.75])\n",
    "        bins = [-np.inf, q25, q50, q75, np.inf]\n",
    "        labels = [f'<{q25:.1f}', f'{q25:.1f}-{q50:.1f}',\n",
    "                 f'{q50:.1f}-{q75:.1f}', f'>{q75:.1f}']\n",
    "\n",
    "    return bins, labels\n",
    "\n",
    "# Preparar datos para an√°lisis por evento individual\n",
    "df_eventos = df_combined.copy()\n",
    "\n",
    "# Calcular m√©tricas de detecci√≥n por evento\n",
    "df_eventos['detectado_p'] = df_eventos['T-P_gpd_dt'].notna()\n",
    "df_eventos['detectado_s'] = df_eventos['T-S_gpd_dt'].notna()\n",
    "df_eventos['detectado_ambas'] = df_eventos['detectado_p'] & df_eventos['detectado_s']\n",
    "\n",
    "# Clasificar errores temporales\n",
    "df_eventos['error_p_abs'] = df_eventos['error_temporal_P'].abs()\n",
    "df_eventos['error_s_abs'] = df_eventos['error_temporal_S'].abs()\n",
    "\n",
    "# Detecci√≥n exitosa con diferentes tolerancias\n",
    "tolerancias = [1.0, 2.0, 5.0]\n",
    "for tol in tolerancias:\n",
    "    df_eventos[f'exitosa_p_{tol}s'] = (df_eventos['detectado_p'] &\n",
    "                                       (df_eventos['error_p_abs'] <= tol))\n",
    "    df_eventos[f'exitosa_s_{tol}s'] = (df_eventos['detectado_s'] &\n",
    "                                       (df_eventos['error_s_abs'] <= tol))\n",
    "\n",
    "print(f\"‚úì Datos preparados: {len(df_eventos)} eventos\")\n",
    "print(f\"‚úì Rango SNR-P: {df_eventos['SNR-P'].min():.1f} a {df_eventos['SNR-P'].max():.1f} dB\")\n",
    "print(f\"‚úì Rango SNR-S: {df_eventos['SNR-S'].min():.1f} a {df_eventos['SNR-S'].max():.1f} dB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 101,
     "status": "ok",
     "timestamp": 1758209877588,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "KrRahVNEK6fU",
    "outputId": "0809fedd-942a-439d-f751-c105fae89c39"
   },
   "outputs": [],
   "source": [
    "# Funci√≥n para calcular m√©tricas por bins de SNR\n",
    "def calcular_metricas_por_bins_snr(df, columna_snr, fase, tolerancia=1.0, metodo_bins='fijo'):\n",
    "    \"\"\"Calcula m√©tricas de detecci√≥n por bins de SNR\"\"\"\n",
    "\n",
    "    bins, labels = crear_bins_snr(df, columna_snr, metodo_bins)\n",
    "    df_temp = df.dropna(subset=[columna_snr]).copy()\n",
    "    df_temp['bin_snr'] = pd.cut(df_temp[columna_snr], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "    metricas_bins = []\n",
    "\n",
    "    for bin_label in labels:\n",
    "        data_bin = df_temp[df_temp['bin_snr'] == bin_label]\n",
    "        if len(data_bin) == 0:\n",
    "            continue\n",
    "\n",
    "        n_eventos = len(data_bin)\n",
    "\n",
    "        # Preparar columnas seg√∫n la fase\n",
    "        if fase == 'P':\n",
    "            ref_col = 'T-P_ref_dt'\n",
    "            det_col = 'T-P_gpd_dt'\n",
    "            error_col = 'error_temporal_P'\n",
    "        else:  # fase == 'S'\n",
    "            ref_col = 'T-S_ref_dt'\n",
    "            det_col = 'T-S_gpd_dt'\n",
    "            error_col = 'error_temporal_S'\n",
    "\n",
    "        # Eventos de referencia (ground truth)\n",
    "        eventos_ref = data_bin[ref_col].notna().sum()\n",
    "\n",
    "        # Detecciones del modelo\n",
    "        eventos_det = data_bin[det_col].notna().sum()\n",
    "\n",
    "        # C√°lculo de TP, FP, FN\n",
    "        mask_ref = data_bin[ref_col].notna()\n",
    "        mask_det = data_bin[det_col].notna()\n",
    "        mask_both = mask_ref & mask_det\n",
    "\n",
    "        # True Positives: detecciones correctas dentro de tolerancia\n",
    "        errores_abs = data_bin[error_col].abs()\n",
    "        tp = (mask_both & (errores_abs <= tolerancia)).sum()\n",
    "\n",
    "        # False Negatives: referencias no detectadas o detectadas fuera de tolerancia\n",
    "        fn = (mask_ref & ~mask_det).sum() + (mask_both & (errores_abs > tolerancia)).sum()\n",
    "\n",
    "        # False Positives: detecciones sin referencia o fuera de tolerancia\n",
    "        fp = (~mask_ref & mask_det).sum() + (mask_both & (errores_abs > tolerancia)).sum()\n",
    "\n",
    "        # M√©tricas principales\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        # Tasa de detecci√≥n simple\n",
    "        tasa_deteccion = eventos_det / eventos_ref * 100 if eventos_ref > 0 else 0\n",
    "\n",
    "        # Estad√≠sticas de error (solo para detecciones v√°lidas)\n",
    "        errores_validos = data_bin[data_bin[det_col].notna()][error_col].dropna()\n",
    "        if len(errores_validos) > 0:\n",
    "            error_mean = errores_validos.mean()\n",
    "            error_median = errores_validos.median()\n",
    "            error_rms = np.sqrt((errores_validos ** 2).mean())\n",
    "            error_std = errores_validos.std()\n",
    "            error_mae = errores_validos.abs().mean()\n",
    "        else:\n",
    "            error_mean = error_median = error_rms = error_std = error_mae = np.nan\n",
    "\n",
    "        # SNR estad√≠sticas del bin\n",
    "        snr_mean = data_bin[columna_snr].mean()\n",
    "        snr_median = data_bin[columna_snr].median()\n",
    "        snr_std = data_bin[columna_snr].std()\n",
    "\n",
    "        metricas_bins.append({\n",
    "            'bin_snr': bin_label,\n",
    "            'snr_mean': snr_mean,\n",
    "            'snr_median': snr_median,\n",
    "            'snr_std': snr_std,\n",
    "            'n_eventos': n_eventos,\n",
    "            'eventos_ref': eventos_ref,\n",
    "            'eventos_det': eventos_det,\n",
    "            'tp': tp,\n",
    "            'fp': fp,\n",
    "            'fn': fn,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'tasa_deteccion': tasa_deteccion,\n",
    "            'error_mean': error_mean,\n",
    "            'error_median': error_median,\n",
    "            'error_rms': error_rms,\n",
    "            'error_std': error_std,\n",
    "            'error_mae': error_mae\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metricas_bins)\n",
    "\n",
    "# Calcular m√©tricas por bins para ambas fases\n",
    "print(\"Calculando m√©tricas por bins de SNR...\")\n",
    "\n",
    "df_bins_p = calcular_metricas_por_bins_snr(df_eventos, 'SNR-P', 'P', tolerancia=1.0)\n",
    "df_bins_s = calcular_metricas_por_bins_snr(df_eventos, 'SNR-S', 'S', tolerancia=1.0)\n",
    "\n",
    "print(\"‚úì M√©tricas por bins calculadas\")\n",
    "print(f\"‚úì Bins SNR-P: {len(df_bins_p)}\")\n",
    "print(f\"‚úì Bins SNR-S: {len(df_bins_s)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1501,
     "status": "ok",
     "timestamp": 1758209879053,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "aiFUbQCMK9bq",
    "outputId": "2ef49254-37e3-4cd4-a91c-36e260351361"
   },
   "outputs": [],
   "source": [
    "# Visualizaci√≥n 1: M√©tricas principales vs SNR\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 12))\n",
    "\n",
    "# Funci√≥n para plotear m√©tricas por bins\n",
    "def plot_metricas_bins(df_bins, fase, axes_row):\n",
    "    x_pos = range(len(df_bins))\n",
    "    labels = df_bins['bin_snr'].tolist()\n",
    "\n",
    "    # F1-Score\n",
    "    axes_row[0].bar(x_pos, df_bins['f1'], alpha=0.7, color='purple')\n",
    "    axes_row[0].set_title(f'F1-Score vs SNR-{fase}')\n",
    "    axes_row[0].set_ylabel('F1-Score')\n",
    "    axes_row[0].set_xticks(x_pos)\n",
    "    axes_row[0].set_xticklabels(labels, rotation=45)\n",
    "    axes_row[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Recall\n",
    "    axes_row[1].bar(x_pos, df_bins['recall'], alpha=0.7, color='blue')\n",
    "    axes_row[1].set_title(f'Recall vs SNR-{fase}')\n",
    "    axes_row[1].set_ylabel('Recall')\n",
    "    axes_row[1].set_xticks(x_pos)\n",
    "    axes_row[1].set_xticklabels(labels, rotation=45)\n",
    "    axes_row[1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Precision\n",
    "    axes_row[2].bar(x_pos, df_bins['precision'], alpha=0.7, color='green')\n",
    "    axes_row[2].set_title(f'Precision vs SNR-{fase}')\n",
    "    axes_row[2].set_ylabel('Precision')\n",
    "    axes_row[2].set_xticks(x_pos)\n",
    "    axes_row[2].set_xticklabels(labels, rotation=45)\n",
    "    axes_row[2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Error RMS\n",
    "    axes_row[3].bar(x_pos, df_bins['error_rms'], alpha=0.7, color='red')\n",
    "    axes_row[3].set_title(f'Error RMS vs SNR-{fase}')\n",
    "    axes_row[3].set_ylabel('Error RMS (s)')\n",
    "    axes_row[3].set_xticks(x_pos)\n",
    "    axes_row[3].set_xticklabels(labels, rotation=45)\n",
    "    axes_row[3].grid(True, alpha=0.3)\n",
    "\n",
    "# Plotear para ambas fases\n",
    "plot_metricas_bins(df_bins_p, 'P', axes[0])\n",
    "plot_metricas_bins(df_bins_s, 'S', axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mostrar tablas num√©ricas\n",
    "print(\"\\n=== M√âTRICAS POR BINS SNR-P ===\")\n",
    "columnas_mostrar = ['bin_snr', 'snr_mean', 'n_eventos', 'f1', 'recall', 'precision', 'error_rms', 'error_median']\n",
    "print(df_bins_p[columnas_mostrar].round(3))\n",
    "\n",
    "print(\"\\n=== M√âTRICAS POR BINS SNR-S ===\")\n",
    "print(df_bins_s[columnas_mostrar].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3344,
     "status": "ok",
     "timestamp": 1758209882398,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "HsEZ4XgiLSPQ",
    "outputId": "94701b48-7f0a-45ec-957f-5b43882182f7"
   },
   "outputs": [],
   "source": [
    "# Visualizaci√≥n 2: An√°lisis de umbrales cr√≠ticos\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# An√°lisis continuo usando ventanas deslizantes\n",
    "def analisis_continuo_snr(df, columna_snr, fase, ventana=50):\n",
    "    \"\"\"An√°lisis continuo de m√©tricas vs SNR usando ventanas deslizantes\"\"\"\n",
    "\n",
    "    df_sorted = df.dropna(subset=[columna_snr]).sort_values(columna_snr)\n",
    "\n",
    "    if fase == 'P':\n",
    "        det_col = 'detectado_p'\n",
    "        error_col = 'error_temporal_P'\n",
    "    else:\n",
    "        det_col = 'detectado_s'\n",
    "        error_col = 'error_temporal_S'\n",
    "\n",
    "    snr_values = []\n",
    "    recall_values = []\n",
    "    precision_values = []\n",
    "    f1_values = []\n",
    "    error_rms_values = []\n",
    "    error_median_values = []\n",
    "\n",
    "    for i in range(len(df_sorted) - ventana + 1):\n",
    "        ventana_data = df_sorted.iloc[i:i+ventana]\n",
    "\n",
    "        snr_medio = ventana_data[columna_snr].mean()\n",
    "\n",
    "        # Calcular recall (tasa de detecci√≥n en esta ventana)\n",
    "        recall = ventana_data[det_col].mean()\n",
    "\n",
    "        # Para precision y F1, necesitamos m√°s l√≥gica (simplificado aqu√≠)\n",
    "        detecciones = ventana_data[det_col].sum()\n",
    "        errores_valid = ventana_data[ventana_data[det_col]][error_col].dropna()\n",
    "\n",
    "        if len(errores_valid) > 0:\n",
    "            # Precision aproximada (detecciones \"buenas\" / total detecciones)\n",
    "            precision_aprox = (errores_valid.abs() <= 1.0).mean()\n",
    "            f1_aprox = 2 * recall * precision_aprox / (recall + precision_aprox) if (recall + precision_aprox) > 0 else 0\n",
    "\n",
    "            error_rms = np.sqrt((errores_valid ** 2).mean())\n",
    "            error_median = errores_valid.median()\n",
    "        else:\n",
    "            precision_aprox = 0\n",
    "            f1_aprox = 0\n",
    "            error_rms = np.nan\n",
    "            error_median = np.nan\n",
    "\n",
    "        snr_values.append(snr_medio)\n",
    "        recall_values.append(recall)\n",
    "        precision_values.append(precision_aprox)\n",
    "        f1_values.append(f1_aprox)\n",
    "        error_rms_values.append(error_rms)\n",
    "        error_median_values.append(error_median)\n",
    "\n",
    "    return (np.array(snr_values), np.array(recall_values), np.array(precision_values),\n",
    "            np.array(f1_values), np.array(error_rms_values), np.array(error_median_values))\n",
    "\n",
    "# An√°lisis continuo para ambas fases\n",
    "if len(df_eventos) >= 100:  # Solo si hay suficientes datos\n",
    "    ventana = min(100, len(df_eventos) // 10)\n",
    "\n",
    "    snr_p, recall_p, prec_p, f1_p, rms_p, med_p = analisis_continuo_snr(df_eventos, 'SNR-P', 'P', ventana)\n",
    "    snr_s, recall_s, prec_s, f1_s, rms_s, med_s = analisis_continuo_snr(df_eventos, 'SNR-S', 'S', ventana)\n",
    "\n",
    "    # Plot an√°lisis continuo\n",
    "    # Fila 1: Fase P\n",
    "    axes[0,0].plot(snr_p, f1_p, 'b-', alpha=0.7, linewidth=2)\n",
    "    axes[0,0].set_xlabel('SNR-P (dB)')\n",
    "    axes[0,0].set_ylabel('F1-Score')\n",
    "    axes[0,0].set_title('F1-Score vs SNR-P (Continuo)')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0,1].plot(snr_p, recall_p, 'g-', alpha=0.7, linewidth=2)\n",
    "    axes[0,1].set_xlabel('SNR-P (dB)')\n",
    "    axes[0,1].set_ylabel('Recall')\n",
    "    axes[0,1].set_title('Recall vs SNR-P (Continuo)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[0,2].plot(snr_p, rms_p, 'r-', alpha=0.7, linewidth=2)\n",
    "    axes[0,2].set_xlabel('SNR-P (dB)')\n",
    "    axes[0,2].set_ylabel('Error RMS (s)')\n",
    "    axes[0,2].set_title('Error RMS vs SNR-P (Continuo)')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "    # Fila 2: Fase S\n",
    "    axes[1,0].plot(snr_s, f1_s, 'b-', alpha=0.7, linewidth=2)\n",
    "    axes[1,0].set_xlabel('SNR-S (dB)')\n",
    "    axes[1,0].set_ylabel('F1-Score')\n",
    "    axes[1,0].set_title('F1-Score vs SNR-S (Continuo)')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1,1].plot(snr_s, recall_s, 'g-', alpha=0.7, linewidth=2)\n",
    "    axes[1,1].set_xlabel('SNR-S (dB)')\n",
    "    axes[1,1].set_ylabel('Recall')\n",
    "    axes[1,1].set_title('Recall vs SNR-S (Continuo)')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "    axes[1,2].plot(snr_s, rms_s, 'r-', alpha=0.7, linewidth=2)\n",
    "    axes[1,2].set_xlabel('SNR-S (dB)')\n",
    "    axes[1,2].set_ylabel('Error RMS (s)')\n",
    "    axes[1,2].set_title('Error RMS vs SNR-S (Continuo)')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "else:\n",
    "    # Si no hay suficientes datos, usar scatter plots simples\n",
    "    for i in range(2):\n",
    "        for j in range(3):\n",
    "            axes[i,j].text(0.5, 0.5, 'Datos insuficientes\\npara an√°lisis continuo',\n",
    "                          ha='center', va='center', transform=axes[i,j].transAxes)\n",
    "            axes[i,j].set_xticks([])\n",
    "            axes[i,j].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758209882403,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "rMhXU276L6Fq",
    "outputId": "31644858-be65-45bf-f0ba-c4adb579c723"
   },
   "outputs": [],
   "source": [
    "# An√°lisis de umbrales cr√≠ticos\n",
    "print(\"\\n=== AN√ÅLISIS DE UMBRALES CR√çTICOS ===\")\n",
    "\n",
    "def encontrar_umbrales_criticos(df_bins, metrica='recall', umbral=0.8):\n",
    "    \"\"\"Encuentra el SNR m√≠nimo para mantener una m√©trica por encima del umbral\"\"\"\n",
    "\n",
    "    # Ordenar por SNR medio\n",
    "    df_sorted = df_bins.sort_values('snr_mean')\n",
    "\n",
    "    # Encontrar primer bin que supera el umbral\n",
    "    bins_sobre_umbral = df_sorted[df_sorted[metrica] >= umbral]\n",
    "\n",
    "    if len(bins_sobre_umbral) > 0:\n",
    "        snr_critico = bins_sobre_umbral.iloc[0]['snr_mean']\n",
    "        return snr_critico, bins_sobre_umbral.iloc[0]\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# Umbrales para diferentes m√©tricas\n",
    "umbrales_analisis = [\n",
    "    ('Recall ‚â• 80%', 'recall', 0.8),\n",
    "    ('Precision ‚â• 70%', 'precision', 0.7),\n",
    "    ('F1-Score ‚â• 70%', 'f1', 0.7),\n",
    "    ('Error RMS ‚â§ 2s', 'error_rms', 2.0)  # Para esta m√©trica, buscamos por debajo\n",
    "]\n",
    "\n",
    "print(\"FASE P:\")\n",
    "for nombre, metrica, umbral in umbrales_analisis:\n",
    "    if metrica == 'error_rms':\n",
    "        # Para error RMS, buscamos el √∫ltimo bin por debajo del umbral\n",
    "        df_sorted = df_bins_p.sort_values('snr_mean')\n",
    "        bins_bajo_umbral = df_sorted[df_sorted[metrica] <= umbral]\n",
    "        if len(bins_bajo_umbral) > 0:\n",
    "            snr_critico = bins_bajo_umbral.iloc[-1]['snr_mean']\n",
    "            print(f\"  {nombre}: SNR-P ‚â• {snr_critico:.1f} dB\")\n",
    "        else:\n",
    "            print(f\"  {nombre}: No se alcanza el umbral\")\n",
    "    else:\n",
    "        snr_critico, bin_info = encontrar_umbrales_criticos(df_bins_p, metrica, umbral)\n",
    "        if snr_critico:\n",
    "            print(f\"  {nombre}: SNR-P ‚â• {snr_critico:.1f} dB\")\n",
    "        else:\n",
    "            print(f\"  {nombre}: No se alcanza el umbral\")\n",
    "\n",
    "print(\"\\nFASE S:\")\n",
    "for nombre, metrica, umbral in umbrales_analisis:\n",
    "    if metrica == 'error_rms':\n",
    "        df_sorted = df_bins_s.sort_values('snr_mean')\n",
    "        bins_bajo_umbral = df_sorted[df_sorted[metrica] <= umbral]\n",
    "        if len(bins_bajo_umbral) > 0:\n",
    "            snr_critico = bins_bajo_umbral.iloc[-1]['snr_mean']\n",
    "            print(f\"  {nombre}: SNR-S ‚â• {snr_critico:.1f} dB\")\n",
    "        else:\n",
    "            print(f\"  {nombre}: No se alcanza el umbral\")\n",
    "    else:\n",
    "        snr_critico, bin_info = encontrar_umbrales_criticos(df_bins_s, metrica, umbral)\n",
    "        if snr_critico:\n",
    "            print(f\"  {nombre}: SNR-S ‚â• {snr_critico:.1f} dB\")\n",
    "        else:\n",
    "            print(f\"  {nombre}: No se alcanza el umbral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1758209882409,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "jLHTw7V5MAm1",
    "outputId": "a7f0c3ff-1cbe-47af-a8c0-42d6ea135891"
   },
   "outputs": [],
   "source": [
    "# An√°lisis de correlaciones\n",
    "print(\"\\n=== AN√ÅLISIS DE CORRELACIONES ===\")\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Preparar datos para correlaciones\n",
    "datos_correlacion = df_eventos.dropna(subset=['SNR-P', 'SNR-S']).copy()\n",
    "\n",
    "# Calcular correlaciones para eventos con detecciones v√°lidas\n",
    "correlaciones = []\n",
    "\n",
    "metricas_corr = [\n",
    "    ('SNR-P', 'detectado_p', 'Detecci√≥n P'),\n",
    "    ('SNR-P', 'error_p_abs', 'Error absoluto P'),\n",
    "    ('SNR-S', 'detectado_s', 'Detecci√≥n S'),\n",
    "    ('SNR-S', 'error_s_abs', 'Error absoluto S')\n",
    "]\n",
    "\n",
    "for snr_col, metrica_col, nombre in metricas_corr:\n",
    "    if metrica_col in datos_correlacion.columns:\n",
    "        datos_validos = datos_correlacion.dropna(subset=[snr_col, metrica_col])\n",
    "\n",
    "        if len(datos_validos) > 10:\n",
    "            corr_pearson, p_pearson = pearsonr(datos_validos[snr_col], datos_validos[metrica_col])\n",
    "            corr_spearman, p_spearman = spearmanr(datos_validos[snr_col], datos_validos[metrica_col])\n",
    "\n",
    "            correlaciones.append({\n",
    "                'Relaci√≥n': f'{snr_col} vs {nombre}',\n",
    "                'N': len(datos_validos),\n",
    "                'Pearson_r': corr_pearson,\n",
    "                'Pearson_p': p_pearson,\n",
    "                'Spearman_rho': corr_spearman,\n",
    "                'Spearman_p': p_spearman,\n",
    "                'Significativa': 'S√≠' if min(p_pearson, p_spearman) < 0.05 else 'No'\n",
    "            })\n",
    "\n",
    "df_correlaciones = pd.DataFrame(correlaciones)\n",
    "print(\"Correlaciones SNR vs M√©tricas del Modelo:\")\n",
    "print(df_correlaciones.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758209882413,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "mgxGeHACMFhR",
    "outputId": "6ec8c257-6917-4572-ce85-5baca44faf1d"
   },
   "outputs": [],
   "source": [
    "# An√°lisis de sesgos sistem√°ticos por rangos de SNR\n",
    "print(\"\\n=== AN√ÅLISIS DE SESGOS SISTEM√ÅTICOS ===\")\n",
    "\n",
    "def analizar_sesgos_por_snr(df, columna_snr, fase):\n",
    "    \"\"\"Analiza sesgos sistem√°ticos en diferentes rangos de SNR\"\"\"\n",
    "\n",
    "    if fase == 'P':\n",
    "        error_col = 'error_temporal_P'\n",
    "    else:\n",
    "        error_col = 'error_temporal_S'\n",
    "\n",
    "    # Crear bins cuartiles\n",
    "    q25, q50, q75 = df[columna_snr].quantile([0.25, 0.5, 0.75])\n",
    "    bins = [-np.inf, q25, q50, q75, np.inf]\n",
    "    labels = [f'Q1 (<{q25:.1f})', f'Q2 ({q25:.1f}-{q50:.1f})',\n",
    "             f'Q3 ({q50:.1f}-{q75:.1f})', f'Q4 (>{q75:.1f})']\n",
    "\n",
    "    df_temp = df.dropna(subset=[columna_snr, error_col]).copy()\n",
    "    df_temp['cuartil_snr'] = pd.cut(df_temp[columna_snr], bins=bins, labels=labels)\n",
    "\n",
    "    sesgos = []\n",
    "    for cuartil in labels:\n",
    "        data_cuartil = df_temp[df_temp['cuartil_snr'] == cuartil][error_col]\n",
    "\n",
    "        if len(data_cuartil) > 5:\n",
    "            sesgo_medio = data_cuartil.mean()\n",
    "            sesgo_mediano = data_cuartil.median()\n",
    "            desv_std = data_cuartil.std()\n",
    "            n_eventos = len(data_cuartil)\n",
    "\n",
    "            # Test de sesgo (¬øes significativamente diferente de 0?)\n",
    "            from scipy.stats import ttest_1samp\n",
    "            t_stat, p_value = ttest_1samp(data_cuartil, 0)\n",
    "\n",
    "            sesgos.append({\n",
    "                'Cuartil': cuartil,\n",
    "                'N': n_eventos,\n",
    "                'Sesgo_medio': sesgo_medio,\n",
    "                'Sesgo_mediano': sesgo_mediano,\n",
    "                'Desv_std': desv_std,\n",
    "                'T_stat': t_stat,\n",
    "                'P_value': p_value,\n",
    "                'Sesgo_significativo': 'S√≠' if p_value < 0.05 else 'No'\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(sesgos)\n",
    "\n",
    "# Analizar sesgos para ambas fases\n",
    "print(\"SESGOS TEMPORALES POR CUARTILES DE SNR-P:\")\n",
    "sesgos_p = analizar_sesgos_por_snr(df_eventos, 'SNR-P', 'P')\n",
    "print(sesgos_p.round(3))\n",
    "\n",
    "print(\"\\nSESGOS TEMPORALES POR CUARTILES DE SNR-S:\")\n",
    "sesgos_s = analizar_sesgos_por_snr(df_eventos, 'SNR-S', 'S')\n",
    "print(sesgos_s.round(3))\n",
    "\n",
    "# Guardar todos los resultados\n",
    "print(f\"\\nüíæ Guardando resultados del an√°lisis...\")\n",
    "df_bins_p.to_csv('/content/analisis_snr_bins_fase_P.csv', index=False)\n",
    "df_bins_s.to_csv('/content/analisis_snr_bins_fase_S.csv', index=False)\n",
    "df_correlaciones.to_csv('/content/correlaciones_snr_metricas.csv', index=False)\n",
    "sesgos_p.to_csv('/content/sesgos_temporales_snr_P.csv', index=False)\n",
    "sesgos_s.to_csv('/content/sesgos_temporales_snr_S.csv', index=False)\n",
    "\n",
    "print(\"‚úì An√°lisis completo guardado en m√∫ltiples archivos CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8cgVPRpJbJZt"
   },
   "source": [
    "# Nueva secci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36,
     "status": "ok",
     "timestamp": 1758209882450,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "65jfSlreMKdA",
    "outputId": "365fe224-04ef-4fca-fdaa-88d48426609b"
   },
   "outputs": [],
   "source": [
    "# Resumen ejecutivo y conclusiones\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RESUMEN EJECUTIVO: RELACI√ìN SNR vs M√âTRICAS DEL MODELO GPD\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Encontrar tendencias principales\n",
    "if len(df_bins_p) > 1:\n",
    "    # Tendencia F1-Score\n",
    "    f1_min_p = df_bins_p['f1'].min()\n",
    "    f1_max_p = df_bins_p['f1'].max()\n",
    "    mejora_f1_p = ((f1_max_p - f1_min_p) / f1_min_p * 100) if f1_min_p > 0 else 0\n",
    "\n",
    "    # Tendencia Recall\n",
    "    recall_min_p = df_bins_p['recall'].min()\n",
    "    recall_max_p = df_bins_p['recall'].max()\n",
    "    mejora_recall_p = ((recall_max_p - recall_min_p) / recall_min_p * 100) if recall_min_p > 0 else 0\n",
    "\n",
    "    print(f\"üìä HALLAZGOS PRINCIPALES FASE P:\")\n",
    "    print(f\"  ‚Ä¢ F1-Score var√≠a de {f1_min_p:.3f} a {f1_max_p:.3f} ({mejora_f1_p:+.1f}% mejora)\")\n",
    "    print(f\"  ‚Ä¢ Recall var√≠a de {recall_min_p:.3f} a {recall_max_p:.3f} ({mejora_recall_p:+.1f}% mejora)\")\n",
    "\n",
    "    # Bin con mejor rendimiento\n",
    "    mejor_bin_p = df_bins_p.loc[df_bins_p['f1'].idxmax()]\n",
    "    print(f\"  ‚Ä¢ Mejor rendimiento en bin: {mejor_bin_p['bin_snr']} (SNR ~{mejor_bin_p['snr_mean']:.1f} dB)\")\n",
    "\n",
    "# Continuar con el resumen ejecutivo\n",
    "if len(df_bins_s) > 1:\n",
    "    f1_min_s = df_bins_s['f1'].min()\n",
    "    f1_max_s = df_bins_s['f1'].max()\n",
    "    mejora_f1_s = ((f1_max_s - f1_min_s) / f1_min_s * 100) if f1_min_s > 0 else 0\n",
    "\n",
    "    recall_min_s = df_bins_s['recall'].min()\n",
    "    recall_max_s = df_bins_s['recall'].max()\n",
    "    mejora_recall_s = ((recall_max_s - recall_min_s) / recall_min_s * 100) if recall_min_s > 0 else 0\n",
    "\n",
    "    print(f\"\\nüìä HALLAZGOS PRINCIPALES FASE S:\")\n",
    "    print(f\"  ‚Ä¢ F1-Score var√≠a de {f1_min_s:.3f} a {f1_max_s:.3f} ({mejora_f1_s:+.1f}% mejora)\")\n",
    "    print(f\"  ‚Ä¢ Recall var√≠a de {recall_min_s:.3f} a {recall_max_s:.3f} ({mejora_recall_s:+.1f}% mejora)\")\n",
    "\n",
    "    mejor_bin_s = df_bins_s.loc[df_bins_s['f1'].idxmax()]\n",
    "    print(f\"  ‚Ä¢ Mejor rendimiento en bin: {mejor_bin_s['bin_snr']} (SNR ~{mejor_bin_s['snr_mean']:.1f} dB)\")\n",
    "\n",
    "# An√°lisis de degradaci√≥n cr√≠tica\n",
    "print(f\"\\nüîç AN√ÅLISIS DE DEGRADACI√ìN CR√çTICA:\")\n",
    "\n",
    "# Encontrar puntos de quiebre en recall\n",
    "def encontrar_punto_quiebre(df_bins, metrica='recall', umbral_critico=0.5):\n",
    "    \"\"\"Encuentra el punto donde la m√©trica cae dr√°sticamente\"\"\"\n",
    "    df_sorted = df_bins.sort_values('snr_mean')\n",
    "\n",
    "    for i in range(len(df_sorted)-1):\n",
    "        actual = df_sorted.iloc[i][metrica]\n",
    "        siguiente = df_sorted.iloc[i+1][metrica]\n",
    "\n",
    "        if actual < umbral_critico and siguiente >= umbral_critico:\n",
    "            return df_sorted.iloc[i+1]['snr_mean'], df_sorted.iloc[i+1]\n",
    "\n",
    "    return None, None\n",
    "\n",
    "# An√°lisis para ambas fases\n",
    "for fase, df_bins in [('P', df_bins_p), ('S', df_bins_s)]:\n",
    "    if len(df_bins) > 1:\n",
    "        # Punto donde recall cae por debajo del 50%\n",
    "        snr_critico_50, bin_info = encontrar_punto_quiebre(df_bins, 'recall', 0.5)\n",
    "        if snr_critico_50:\n",
    "            print(f\"  ‚Ä¢ Fase {fase}: Recall >50% requiere SNR ‚â• {snr_critico_50:.1f} dB\")\n",
    "\n",
    "        # Punto donde F1 cae por debajo del 50%\n",
    "        snr_critico_f1, _ = encontrar_punto_quiebre(df_bins, 'f1', 0.5)\n",
    "        if snr_critico_f1:\n",
    "            print(f\"  ‚Ä¢ Fase {fase}: F1-Score >50% requiere SNR ‚â• {snr_critico_f1:.1f} dB\")\n",
    "\n",
    "# An√°lisis de precisi√≥n vs recall trade-off\n",
    "print(f\"\\n‚öñÔ∏è AN√ÅLISIS PRECISION vs RECALL:\")\n",
    "\n",
    "for fase, df_bins in [('P', df_bins_p), ('S', df_bins_s)]:\n",
    "    if len(df_bins) > 1:\n",
    "        # Correlaci√≥n entre precision y recall por bins\n",
    "        if len(df_bins) >= 3:\n",
    "            corr_pr = df_bins['precision'].corr(df_bins['recall'])\n",
    "            print(f\"  ‚Ä¢ Fase {fase}: Correlaci√≥n Precision-Recall = {corr_pr:.3f}\")\n",
    "\n",
    "        # Identificar bins con alta precision pero bajo recall (conservador)\n",
    "        conservadores = df_bins[(df_bins['precision'] > 0.7) & (df_bins['recall'] < 0.5)]\n",
    "        if len(conservadores) > 0:\n",
    "            print(f\"  ‚Ä¢ Fase {fase}: {len(conservadores)} bins muestran comportamiento conservador (alta precision, bajo recall)\")\n",
    "\n",
    "        # Identificar bins con bajo precision pero alto recall (liberal)\n",
    "        liberales = df_bins[(df_bins['precision'] < 0.5) & (df_bins['recall'] > 0.7)]\n",
    "        if len(liberales) > 0:\n",
    "            print(f\"  ‚Ä¢ Fase {fase}: {len(liberales)} bins muestran comportamiento liberal (bajo precision, alto recall)\")\n",
    "\n",
    "print(f\"\\nüéØ RECOMENDACIONES T√âCNICAS:\")\n",
    "\n",
    "# Recomendaciones basadas en hallazgos\n",
    "recomendaciones = []\n",
    "\n",
    "# Recomendaciones de SNR m√≠nimo\n",
    "if len(df_bins_p) > 1:\n",
    "    bins_buenos_p = df_bins_p[df_bins_p['f1'] > 0.6]\n",
    "    if len(bins_buenos_p) > 0:\n",
    "        snr_min_recomendado_p = bins_buenos_p['snr_mean'].min()\n",
    "        recomendaciones.append(f\"SNR-P m√≠nimo recomendado: {snr_min_recomendado_p:.1f} dB para F1>0.6\")\n",
    "\n",
    "if len(df_bins_s) > 1:\n",
    "    bins_buenos_s = df_bins_s[df_bins_s['f1'] > 0.6]\n",
    "    if len(bins_buenos_s) > 0:\n",
    "        snr_min_recomendado_s = bins_buenos_s['snr_mean'].min()\n",
    "        recomendaciones.append(f\"SNR-S m√≠nimo recomendado: {snr_min_recomendado_s:.1f} dB para F1>0.6\")\n",
    "\n",
    "# Recomendaciones de preprocesamiento\n",
    "if len(df_correlaciones) > 0:\n",
    "    corr_significativas = df_correlaciones[df_correlaciones['Significativa'] == 'S√≠']\n",
    "    if len(corr_significativas) > 0:\n",
    "        recomendaciones.append(\"Implementar filtrado adaptativo basado en SNR estimado en tiempo real\")\n",
    "        recomendaciones.append(\"Considerar umbrales de detecci√≥n variables seg√∫n SNR de la estaci√≥n\")\n",
    "\n",
    "# Recomendaciones de calidad de datos\n",
    "problemas_snr_bajo = df_eventos[df_eventos['SNR-P'] < 10]['Estacion'].value_counts()\n",
    "if len(problemas_snr_bajo) > 0:\n",
    "    estacion_mas_problematica = problemas_snr_bajo.index[0]\n",
    "    eventos_problematicos = problemas_snr_bajo.iloc[0]\n",
    "    recomendaciones.append(f\"Revisar estaci√≥n {estacion_mas_problematica} ({eventos_problematicos} eventos con SNR-P<10)\")\n",
    "\n",
    "# Imprimir recomendaciones\n",
    "for i, rec in enumerate(recomendaciones, 1):\n",
    "    print(f\"  {i}. {rec}\")\n",
    "\n",
    "# Recomendaciones para mejora del modelo\n",
    "print(f\"\\nüîß ESTRATEGIAS PARA MEJORAR EL MODELO GPD:\")\n",
    "estrategias = [\n",
    "    \"Implementar umbrales adaptativos por estaci√≥n basados en SNR hist√≥rico\",\n",
    "    \"Desarrollar m√≥dulo de estimaci√≥n de SNR en tiempo real\",\n",
    "    \"Crear modelos especializados para diferentes rangos de SNR\",\n",
    "    \"Implementar post-procesamiento para corregir sesgos temporales en SNR bajo\",\n",
    "    \"Considerar ensemble de modelos optimizados para diferentes condiciones de SNR\",\n",
    "    \"Desarrollar m√©tricas de confianza basadas en SNR para filtrar detecciones inciertas\"\n",
    "]\n",
    "\n",
    "for i, estrategia in enumerate(estrategias, 1):\n",
    "    print(f\"  {i}. {estrategia}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1758209883296,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "aVeCPqHLMmq4",
    "outputId": "96861ee3-7ff2-4f8a-b7a2-5ff15820d284"
   },
   "outputs": [],
   "source": [
    "# Visualizaci√≥n final: Heatmap resumen\n",
    "print(f\"\\nüìà GENERANDO VISUALIZACI√ìN RESUMEN...\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Preparar datos para heatmaps\n",
    "def crear_matriz_resumen(df_bins, metricas=['f1', 'recall', 'precision', 'error_rms']):\n",
    "    \"\"\"Crea matriz para heatmap con m√©tricas por bins de SNR\"\"\"\n",
    "    if len(df_bins) == 0:\n",
    "        return None, None\n",
    "\n",
    "    matriz = []\n",
    "    labels_bins = []\n",
    "\n",
    "    for _, row in df_bins.iterrows():\n",
    "        fila = [row[m] for m in metricas]\n",
    "        matriz.append(fila)\n",
    "        labels_bins.append(row['bin_snr'])\n",
    "\n",
    "    return np.array(matriz), labels_bins\n",
    "\n",
    "# Heatmap Fase P\n",
    "matriz_p, labels_p = crear_matriz_resumen(df_bins_p)\n",
    "if matriz_p is not None:\n",
    "    im1 = axes[0,0].imshow(matriz_p.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[0,0].set_title('M√©tricas Fase P por Bins de SNR')\n",
    "    axes[0,0].set_xticks(range(len(labels_p)))\n",
    "    axes[0,0].set_xticklabels(labels_p, rotation=45)\n",
    "    axes[0,0].set_yticks(range(4))\n",
    "    axes[0,0].set_yticklabels(['F1-Score', 'Recall', 'Precision', 'Error RMS'])\n",
    "\n",
    "    # A√±adir valores en las celdas\n",
    "    for i in range(len(labels_p)):\n",
    "        for j in range(4):\n",
    "            if j < 3:  # Para F1, Recall, Precision\n",
    "                texto = f'{matriz_p[i,j]:.2f}'\n",
    "            else:  # Para Error RMS\n",
    "                texto = f'{matriz_p[i,j]:.1f}s'\n",
    "            axes[0,0].text(i, j, texto, ha='center', va='center',\n",
    "                          color='white' if matriz_p[i,j] < 0.5 else 'black', fontsize=9)\n",
    "\n",
    "# Heatmap Fase S\n",
    "matriz_s, labels_s = crear_matriz_resumen(df_bins_s)\n",
    "if matriz_s is not None:\n",
    "    im2 = axes[0,1].imshow(matriz_s.T, cmap='RdYlGn', aspect='auto', vmin=0, vmax=1)\n",
    "    axes[0,1].set_title('M√©tricas Fase S por Bins de SNR')\n",
    "    axes[0,1].set_xticks(range(len(labels_s)))\n",
    "    axes[0,1].set_xticklabels(labels_s, rotation=45)\n",
    "    axes[0,1].set_yticks(range(4))\n",
    "    axes[0,1].set_yticklabels(['F1-Score', 'Recall', 'Precision', 'Error RMS'])\n",
    "\n",
    "    for i in range(len(labels_s)):\n",
    "        for j in range(4):\n",
    "            if j < 3:\n",
    "                texto = f'{matriz_s[i,j]:.2f}'\n",
    "            else:\n",
    "                texto = f'{matriz_s[i,j]:.1f}s'\n",
    "            axes[0,1].text(i, j, texto, ha='center', va='center',\n",
    "                          color='white' if matriz_s[i,j] < 0.5 else 'black', fontsize=9)\n",
    "\n",
    "# Scatter combinado: SNR vs F1 para ambas fases\n",
    "if len(df_bins_p) > 0 and len(df_bins_s) > 0:\n",
    "    axes[1,0].scatter(df_bins_p['snr_mean'], df_bins_p['f1'],\n",
    "                     label='Fase P', alpha=0.8, s=100, color='blue')\n",
    "    axes[1,0].scatter(df_bins_s['snr_mean'], df_bins_s['f1'],\n",
    "                     label='Fase S', alpha=0.8, s=100, color='red')\n",
    "\n",
    "    axes[1,0].set_xlabel('SNR Medio (dB)')\n",
    "    axes[1,0].set_ylabel('F1-Score')\n",
    "    axes[1,0].set_title('F1-Score vs SNR (Ambas Fases)')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "    # L√≠neas de tendencia\n",
    "    if len(df_bins_p) > 2:\n",
    "        z_p = np.polyfit(df_bins_p['snr_mean'], df_bins_p['f1'], 1)\n",
    "        p_p = np.poly1d(z_p)\n",
    "        x_trend = np.linspace(df_bins_p['snr_mean'].min(), df_bins_p['snr_mean'].max(), 100)\n",
    "        axes[1,0].plot(x_trend, p_p(x_trend), \"b--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "    if len(df_bins_s) > 2:\n",
    "        z_s = np.polyfit(df_bins_s['snr_mean'], df_bins_s['f1'], 1)\n",
    "        p_s = np.poly1d(z_s)\n",
    "        x_trend_s = np.linspace(df_bins_s['snr_mean'].min(), df_bins_s['snr_mean'].max(), 100)\n",
    "        axes[1,0].plot(x_trend_s, p_s(x_trend_s), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# Distribuci√≥n de eventos por bins de SNR\n",
    "if len(df_bins_p) > 0:\n",
    "    axes[1,1].bar(range(len(df_bins_p)), df_bins_p['n_eventos'],\n",
    "                 alpha=0.7, color='skyblue', label='Fase P')\n",
    "    axes[1,1].set_xlabel('Bins de SNR-P')\n",
    "    axes[1,1].set_ylabel('N√∫mero de Eventos')\n",
    "    axes[1,1].set_title('Distribuci√≥n de Eventos por Bins de SNR')\n",
    "    axes[1,1].set_xticks(range(len(df_bins_p)))\n",
    "    axes[1,1].set_xticklabels([f'{b}' for b in df_bins_p['bin_snr']], rotation=45)\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# A√±adir colorbar\n",
    "fig.colorbar(im1 if matriz_p is not None else im2, ax=axes[0,:],\n",
    "             label='Valor de M√©trica', shrink=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1758209883307,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "HczJ2EwsMs0U",
    "outputId": "294cc2a0-d909-467e-d465-6195cb83722f"
   },
   "outputs": [],
   "source": [
    "# An√°lisis predictivo: ¬øQu√© SNR necesito para obtener X performance?\n",
    "print(f\"\\nüîÆ AN√ÅLISIS PREDICTIVO:\")\n",
    "print(f\"¬øQu√© SNR necesito para obtener cierta performance?\")\n",
    "\n",
    "def predecir_snr_necesario(df_bins, metrica_objetivo, valor_objetivo):\n",
    "    \"\"\"Predice el SNR necesario para alcanzar un valor objetivo de m√©trica\"\"\"\n",
    "\n",
    "    if len(df_bins) < 2:\n",
    "        return None\n",
    "\n",
    "    # Ordenar por SNR\n",
    "    df_sorted = df_bins.sort_values('snr_mean')\n",
    "\n",
    "    # Interpolaci√≥n lineal simple\n",
    "    snr_vals = df_sorted['snr_mean'].values\n",
    "    metrica_vals = df_sorted[metrica_objetivo].values\n",
    "\n",
    "    # Encontrar puntos para interpolaci√≥n\n",
    "    indices_validos = ~np.isnan(metrica_vals)\n",
    "    if np.sum(indices_validos) < 2:\n",
    "        return None\n",
    "\n",
    "    snr_clean = snr_vals[indices_validos]\n",
    "    metrica_clean = metrica_vals[indices_validos]\n",
    "\n",
    "    # Interpolaci√≥n\n",
    "    try:\n",
    "        snr_predicho = np.interp(valor_objetivo, metrica_clean, snr_clean)\n",
    "        return snr_predicho\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Objetivos de performance t√≠picos\n",
    "objetivos_performance = [\n",
    "    ('F1-Score = 0.8', 'f1', 0.8),\n",
    "    ('F1-Score = 0.7', 'f1', 0.7),\n",
    "    ('Recall = 0.9', 'recall', 0.9),\n",
    "    ('Recall = 0.8', 'recall', 0.8),\n",
    "    ('Precision = 0.8', 'precision', 0.8),\n",
    "    ('Error RMS = 1.5s', 'error_rms', 1.5)\n",
    "]\n",
    "\n",
    "print(f\"\\nFASE P:\")\n",
    "for objetivo_desc, metrica, valor in objetivos_performance:\n",
    "    if metrica == 'error_rms':\n",
    "        # Para error RMS, necesitamos invertir la l√≥gica\n",
    "        df_invertido = df_bins_p.copy()\n",
    "        df_invertido['error_rms_inv'] = 1.0 / (df_invertido['error_rms'] + 0.1)  # Evitar divisi√≥n por 0\n",
    "        snr_pred = predecir_snr_necesario(df_invertido, 'error_rms_inv', 1.0/(valor + 0.1))\n",
    "    else:\n",
    "        snr_pred = predecir_snr_necesario(df_bins_p, metrica, valor)\n",
    "\n",
    "    if snr_pred:\n",
    "        print(f\"  ‚Ä¢ Para {objetivo_desc}: SNR-P ‚âà {snr_pred:.1f} dB\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ Para {objetivo_desc}: No se puede predecir (datos insuficientes)\")\n",
    "\n",
    "print(f\"\\nFASE S:\")\n",
    "for objetivo_desc, metrica, valor in objetivos_performance:\n",
    "    if metrica == 'error_rms':\n",
    "        df_invertido = df_bins_s.copy()\n",
    "        df_invertido['error_rms_inv'] = 1.0 / (df_invertido['error_rms'] + 0.1)\n",
    "        snr_pred = predecir_snr_necesario(df_invertido, 'error_rms_inv', 1.0/(valor + 0.1))\n",
    "    else:\n",
    "        snr_pred = predecir_snr_necesario(df_bins_s, metrica, valor)\n",
    "\n",
    "    if snr_pred:\n",
    "        print(f\"  ‚Ä¢ Para {objetivo_desc}: SNR-S ‚âà {snr_pred:.1f} dB\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ Para {objetivo_desc}: No se puede predecir (datos insuficientes)\")\n",
    "\n",
    "# Validaci√≥n de hip√≥tesis final\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"VALIDACI√ìN DE HIP√ìTESIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hipotesis_original = \"Un bajo SNR dificulta la detecci√≥n por parte del modelo GPD\"\n",
    "\n",
    "print(f\"üí° HIP√ìTESIS ORIGINAL: {hipotesis_original}\")\n",
    "print(f\"\\n‚úÖ VALIDACI√ìN:\")\n",
    "\n",
    "# Evidencias recolectadas\n",
    "evidencias = []\n",
    "\n",
    "# 1. Correlaci√≥n SNR vs m√©tricas\n",
    "if len(df_correlaciones) > 0:\n",
    "    corr_significativas = df_correlaciones[df_correlaciones['Significativa'] == 'S√≠']\n",
    "    if len(corr_significativas) > 0:\n",
    "        evidencias.append(f\"Correlaciones significativas encontradas entre SNR y {len(corr_significativas)} m√©tricas\")\n",
    "\n",
    "# 2. Variaci√≥n en bins\n",
    "if len(df_bins_p) > 1:\n",
    "    var_f1_p = df_bins_p['f1'].max() - df_bins_p['f1'].min()\n",
    "    if var_f1_p > 0.2:\n",
    "        evidencias.append(f\"F1-Score Fase P var√≠a {var_f1_p:.3f} puntos entre bins de SNR\")\n",
    "\n",
    "if len(df_bins_s) > 1:\n",
    "    var_f1_s = df_bins_s['f1'].max() - df_bins_s['f1'].min()\n",
    "    if var_f1_s > 0.2:\n",
    "        evidencias.append(f\"F1-Score Fase S var√≠a {var_f1_s:.3f} puntos entre bins de SNR\")\n",
    "\n",
    "# 3. Tendencias monot√≥nicas\n",
    "if len(df_bins_p) > 2:\n",
    "    # Verificar si F1 tiende a aumentar con SNR\n",
    "    corr_f1_snr_p = df_bins_p['snr_mean'].corr(df_bins_p['f1'])\n",
    "    if corr_f1_snr_p > 0.5:\n",
    "        evidencias.append(f\"Tendencia positiva fuerte SNR-P vs F1-Score (r={corr_f1_snr_p:.3f})\")\n",
    "\n",
    "if len(df_bins_s) > 2:\n",
    "    corr_f1_snr_s = df_bins_s['snr_mean'].corr(df_bins_s['f1'])\n",
    "    if corr_f1_snr_s > 0.5:\n",
    "        evidencias.append(f\"Tendencia positiva fuerte SNR-S vs F1-Score (r={corr_f1_snr_s:.3f})\")\n",
    "\n",
    "# Imprimir evidencias\n",
    "for i, evidencia in enumerate(evidencias, 1):\n",
    "    print(f\"  {i}. {evidencia}\")\n",
    "\n",
    "# Conclusi√≥n final\n",
    "if len(evidencias) >= 2:\n",
    "    print(f\"\\nüéØ CONCLUSI√ìN: HIP√ìTESIS CONFIRMADA\")\n",
    "    print(f\"   La evidencia cuantitativa respalda fuertemente que el SNR bajo\")\n",
    "    print(f\"   impacta negativamente el rendimiento del modelo GPD.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  CONCLUSI√ìN: EVIDENCIA LIMITADA\")\n",
    "    print(f\"   Se requieren m√°s datos para confirmar completamente la hip√≥tesis.\")\n",
    "\n",
    "print(f\"\\nüíæ TODOS LOS AN√ÅLISIS GUARDADOS EN:\")\n",
    "print(f\"   ‚Ä¢ /content/analisis_snr_bins_fase_P.csv\")\n",
    "print(f\"   ‚Ä¢ /content/analisis_snr_bins_fase_S.csv\")\n",
    "print(f\"   ‚Ä¢ /content/correlaciones_snr_metricas.csv\")\n",
    "print(f\"   ‚Ä¢ /content/sesgos_temporales_snr_P.csv\")\n",
    "print(f\"   ‚Ä¢ /content/sesgos_temporales_snr_S.csv\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*80)\n",
    "print(\"AN√ÅLISIS COMPLETADO\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXBRXc3OoZ8-"
   },
   "source": [
    "Errores en las detecciones de las fases para cada estaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1758209883312,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "elSFMAGToIqi",
    "outputId": "40963cf5-eedf-43fd-d6b9-8cf68b65400b"
   },
   "outputs": [],
   "source": [
    "# Cargar los datos de an√°lisis completo\n",
    "print(\"=== M√âTRICAS POR ESTACI√ìN: SNR Y ERRORES DEL MODELO GPD ===\")\n",
    "\n",
    "# Cargar el archivo con el an√°lisis completo del impacto del modelo\n",
    "df_impacto = pd.read_csv('/content/analisis_completo_impacto_modelo.csv')\n",
    "df_robustez = pd.read_csv('/content/analisis_robustez_estaciones.csv')\n",
    "\n",
    "# Combinar informaci√≥n relevante\n",
    "df_resumen = pd.merge(\n",
    "    df_impacto[['Estacion', 'N_eventos', 'error_p_mean', 'error_p_median', 'error_p_rms',\n",
    "                'error_s_mean', 'error_s_median', 'error_s_rms', 'Clasificacion']],\n",
    "    df_robustez[['Estacion', 'snr_p_mean', 'snr_p_median', 'snr_s_mean', 'snr_s_median']],\n",
    "    on='Estacion'\n",
    ")\n",
    "\n",
    "print(\"‚úì Datos cargados y combinados\")\n",
    "print(f\"‚úì Estaciones analizadas: {len(df_resumen)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1758209883320,
     "user": {
      "displayName": "Rodrigo Mu√±oz",
      "userId": "01896554238733167311"
     },
     "user_tz": 300
    },
    "id": "RTeAA0I0of79",
    "outputId": "d58fbb4c-b6cf-4b75-e17e-f60330c4acd6"
   },
   "outputs": [],
   "source": [
    "# Tabla resumen completa por estaci√≥n\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"RESUMEN COMPLETO POR ESTACI√ìN\")\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Ordenar por clasificaci√≥n (Problem√°tica -> Intermedia -> Robusta)\n",
    "orden_clasificacion = {'Problem√°tica': 1, 'Intermedia': 2, 'Robusta': 3}\n",
    "df_resumen['orden'] = df_resumen['Clasificacion'].map(orden_clasificacion)\n",
    "df_resumen_ordenado = df_resumen.sort_values('orden')\n",
    "\n",
    "# Mostrar tabla completa\n",
    "print(f\"{'Estaci√≥n':<8} {'Clasif':<12} {'N_Evt':<6} {'SNR-P':<15} {'SNR-S':<15} {'Error-P (s)':<25} {'Error-S (s)':<25}\")\n",
    "print(f\"{'':<8} {'':<12} {'':<6} {'Med|Mean':<15} {'Med|Mean':<15} {'Med|Mean|RMS':<25} {'Med|Mean|RMS':<25}\")\n",
    "print(\"-\" * 120)\n",
    "\n",
    "for _, row in df_resumen_ordenado.iterrows():\n",
    "    estacion = row['Estacion']\n",
    "    clasificacion = row['Clasificacion']\n",
    "    n_eventos = int(row['N_eventos'])\n",
    "\n",
    "    # SNR P\n",
    "    snr_p_med = row['snr_p_median']\n",
    "    snr_p_mean = row['snr_p_mean']\n",
    "    snr_p_str = f\"{snr_p_med:.1f}|{snr_p_mean:.1f}\"\n",
    "\n",
    "    # SNR S\n",
    "    snr_s_med = row['snr_s_median']\n",
    "    snr_s_mean = row['snr_s_mean']\n",
    "    snr_s_str = f\"{snr_s_med:.1f}|{snr_s_mean:.1f}\"\n",
    "\n",
    "    # Errores P\n",
    "    if pd.notna(row['error_p_median']) and pd.notna(row['error_p_mean']) and pd.notna(row['error_p_rms']):\n",
    "        error_p_str = f\"{row['error_p_median']:+.2f}|{row['error_p_mean']:+.2f}|{row['error_p_rms']:.2f}\"\n",
    "    else:\n",
    "        error_p_str = \"Sin datos suficientes\"\n",
    "\n",
    "    # Errores S\n",
    "    if pd.notna(row['error_s_median']) and pd.notna(row['error_s_mean']) and pd.notna(row['error_s_rms']):\n",
    "        error_s_str = f\"{row['error_s_median']:+.2f}|{row['error_s_mean']:+.2f}|{row['error_s_rms']:.2f}\"\n",
    "    else:\n",
    "        error_s_str = \"Sin datos suficientes\"\n",
    "\n",
    "    print(f\"{estacion:<8} {clasificacion:<12} {n_eventos:<6} {snr_p_str:<15} {snr_s_str:<15} {error_p_str:<25} {error_s_str:<25}\")\n",
    "\n",
    "print(\"-\" * 120)\n",
    "print(\"Nota: Errores con signo + indican que el modelo detecta TARDE, - indica detecci√≥n TEMPRANA\")\n",
    "print(\"      RMS siempre es positivo y mide la magnitud total del error\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyODp+1Md/iSLNzapH7THuHJ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "jup_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
